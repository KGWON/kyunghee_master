{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement seq2seq\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 모듈을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ku/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint # print() 함수의 결과를 아름답게(beautify) 보여주는 모듈\n",
    "from tools import * # 직접만든 함수 모음.\n",
    "import math\n",
    "from tqdm import tqdm # 배치학습 for문을 예쁘게 시각화 하는 모듈\n",
    "from tqdm import tqdm_notebook\n",
    "import pymysql # mysql과 파이썬을 연동하기 위한 모듈\n",
    "from pulp import * # transportation problem, linear programming과 관련된 모듈\n",
    "\n",
    "tf.reset_default_graph() # 텐서플로우 그래프를 초기화 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. corpus(YOLO의 output)를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([['dallas take off', ' plane take off from dallas'],\n",
      "       ['dallas land', ' plane land at Dallas'],\n",
      "       ['chicago take off', ' plane take off from chicago'],\n",
      "       ['chicago land', ' plane land at chicago'],\n",
      "       ['los angeles take off', ' plane take off from los angeles'],\n",
      "       ['los angeles land', ' plane land at los angeles'],\n",
      "       ['new york take off', ' plane take off from new york'],\n",
      "       ['new york land', ' plane land at new york'],\n",
      "       ['miami take off', ' plane take off from miami'],\n",
      "       ['miami land', ' plane land at miami']], dtype=object), <class 'numpy.ndarray'>)\n",
      "\n",
      "corpus's shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# pandas DataFrame을 numpy.ndarray로 바꾸려면 '.values'만 추가하면 된다.\n",
    "# => pd.read_table(...).values\n",
    "corpus = pd.read_table(\"./raw.csv\", delimiter=\",\").values\n",
    "\n",
    "print((corpus, type(corpus)))\n",
    "print(\"\\ncorpus's shape: {}\".format(corpus.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. lookup table를 생성하고 이를 이용하여 데이터를 인덱스로 맵핑한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq 모델의 cell은 인풋값으로 index로 된 numpy array나 리스트를 받는다.\n",
    "- tools.make_vocab(numpy.ndarray)\n",
    "    - numpy.ndarray: [batch size x time steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 17 \n",
      "\n",
      "idx2word: \n",
      " {0: 'los', 1: 'at', 2: 'land', 3: 'new', 4: 'take', 5: 'from', 6: 'dallas', 7: 'angeles', 8: 'york', 9: 'miami', 10: 'off', 11: 'plane', 12: 'chicago', 13: '<start>', 14: '<end>', 15: '<pad>', 16: '<unk>'} \n",
      "\n",
      "word2idx: \n",
      " {'los': 0, 'at': 1, 'land': 2, 'new': 3, 'take': 4, 'from': 5, 'dallas': 6, 'angeles': 7, 'york': 8, 'miami': 9, 'off': 10, 'plane': 11, 'chicago': 12, '<start>': 13, '<end>': 14, '<pad>': 15, '<unk>': 16} \n",
      "\n",
      "temp_encoder_data: \n",
      " [[6, 4, 10], [6, 2], [12, 4, 10], [12, 2], [0, 7, 4, 10], [0, 7, 2], [3, 8, 4, 10], [3, 8, 2], [9, 4, 10], [9, 2]] \n",
      "\n",
      "temp_decoder_data: \n",
      " [[11, 4, 10, 5, 6], [11, 2, 1, 6], [11, 4, 10, 5, 12], [11, 2, 1, 12], [11, 4, 10, 5, 0, 7], [11, 2, 1, 0, 7], [11, 4, 10, 5, 3, 8], [11, 2, 1, 3, 8], [11, 4, 10, 5, 9], [11, 2, 1, 9]] \n",
      "\n",
      "temp_targetsr_data: \n",
      " [[11, 4, 10, 5, 6], [11, 2, 1, 6], [11, 4, 10, 5, 12], [11, 2, 1, 12], [11, 4, 10, 5, 0, 7], [11, 2, 1, 0, 7], [11, 4, 10, 5, 3, 8], [11, 2, 1, 3, 8], [11, 4, 10, 5, 9], [11, 2, 1, 9]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make idx2word, word2idx, vocab_size\n",
    "idx2word, word2idx, vocab_size = make_vocab(corpus)\n",
    "\n",
    "# make_temp_data(numpy.ndarray, idx2word, word2idx)\n",
    "# => corpus를 인덱스 값으로 변환 시킨후 return한다. (<pad>, <start>, <end>, <unk> 토큰은 삽입되지 않음)\n",
    "# Dimension: batch size X time steps\n",
    "temp_encoder_data, temp_decoder_data, temp_targets_data = make_temp_data(corpus, idx2word=idx2word, word2idx=word2idx)\n",
    "\n",
    "# Identify temp_encoder_data, temp_decoder_data, temp_targets_data\n",
    "print('vocab_size: {} \\n'.format(vocab_size))\n",
    "print('idx2word: \\n {} \\n'.format(idx2word))\n",
    "print('word2idx: \\n {} \\n'.format(word2idx))\n",
    "\n",
    "# 패딩 토큰이 삽입되지 않아서 배치마다 time steps의 길이가 다르다. 이후에 수정 해 줄 것이다.\n",
    "print('temp_encoder_data: \\n {} \\n'.format(temp_encoder_data))\n",
    "print('temp_decoder_data: \\n {} \\n'.format(temp_decoder_data))\n",
    "print('temp_targetsr_data: \\n {} \\n'.format(temp_targets_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 각 데이터에 pad, end 토큰을 삽입하여 최종적인 데이터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tools.insert_tokens(numpy.ndarray, word2idx)\n",
    "# => insert_tokens()의 argument numpy.ndarray는 index로 변환 된 데이터를 받는다. 주의하자.\n",
    "# Dimension: batch size X time step\n",
    "encoder_data, decoder_data, targets_data, max_decoder_steps = insert_tokens(temp_encoder_data, temp_decoder_data, temp_targets_data, word2idx)\n",
    "\n",
    "# 최종적인 데이터가 잘 생성 되었는지를 확인하려면 아래 코드를 실행해보자.(옵션)\n",
    "# data_checker(encoder_data, idx2word)\n",
    "# data_checker(decoder_data, idx2word)\n",
    "# data_checker(targets_data, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. mask를 사용하기 위해서 sequence_length를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 3, 2, 4, 3, 4, 3, 3, 2]\n",
      "[6, 5, 6, 5, 7, 6, 7, 6, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "# 각 배치에서 <pad>를 제외한 실제 의미 있는 time steps의 길이를 구한다.\n",
    "# Dimension: batch size(1-D)\n",
    "encoder_sequence_length = sequence_length_maker(encoder_data, word2idx)\n",
    "decoder_sequence_length = sequence_length_maker(decoder_data, word2idx)\n",
    "\n",
    "print(encoder_sequence_length)\n",
    "print(decoder_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 하이퍼파라미터를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder의 outputs을 projection layer에 통과시키면 batch size X time step X class_size의 dimension을 얻게 된다.\n",
    "# 원-핫 인코딩이면 vocab_size와 같게 해 주어야 한다.\n",
    "class_size = vocab_size\n",
    "batch_size = 3\n",
    "lr = 0.01\n",
    "# seq2seq셀에 흐르는 hidden state의 유닛개수\n",
    "hidden_size = 128\n",
    "epochs = 100\n",
    "save_per_ckpt = 10\n",
    "# stacked RNN을 위한 하이퍼파라미터. 몇개의 레이어를 쌓을지 정해준다.\n",
    "num_layer = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델을 클래스로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object2Text:\n",
    "    # 'btc_': 배치\n",
    "    '''\n",
    "    Arguments:\n",
    "        btc_enc_seq_len: placeholder([None,]), 인코더 부분의 패딩을 제외한 실제 의미를 가지는 토큰의 개수를 배치별로 나타냄.\n",
    "        btc_enc_inp: placeholder([None, None]), 인코더의 인풋(batch size, time steps)\n",
    "        btc_dec_seq_len: placeholder([None,]), 디코더 부분의 패딩을 제외한 실제 의미를 가지는 토큰의 개수를 배치별로 나타냄.\n",
    "        btc_dec_inp: placeholder([Noen, None]), 디코더의 인풋(batch size, time steps)\n",
    "        btc_targets: placeholder([None, None]), groud truth(batch size, time steps)\n",
    "        word2idx: dictionary, '단어'를 키 값으로 조회하면 그에 해당하는 인덱스를 돌려주는 딕셔너리.\n",
    "        idx2word: dictionary, '인덱스'를 키 값으로 조회하면 그에 해당하는 인덱스를 돌려주는 딕셔너리.\n",
    "        vocab_size: scalar, 단어사전의 크기(단어사전에 있는 단어의 개수)\n",
    "        class_size: scalar, 디코더의 아웃풋을 fully connected 레이어와 연결하여 최종적으로 몇개의 클래스로 나타낼지 결정. 일반적으로 vocab size와 같게 하면 된다.\n",
    "        hidden_size: scalar, seq2seq 모델의 히든 유닛의 개수\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets, word2idx=word2idx, idx2word=idx2word, \n",
    "                     vocab_size=vocab_size, class_size=class_size, hidden_size=hidden_size, num_layer=num_layer, max_decoder_steps=max_decoder_steps):\n",
    "        \n",
    "        # 생성자의 인자들을 클래스멤버(?)로 만들어준다.\n",
    "        self._enc_seq_len = btc_enc_seq_len\n",
    "        self._enc_inp = btc_enc_inp\n",
    "        self._dec_seq_len = btc_dec_seq_len\n",
    "        self._dec_inp = btc_dec_inp\n",
    "        self._targets = btc_targets\n",
    "        self._word2idx = word2idx\n",
    "        self._idx2word = idx2word\n",
    "        self._vocab_size = vocab_size\n",
    "        self._class_size = class_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layer = num_layer\n",
    "        self._max_decoder_steps = max_decoder_steps\n",
    "    \n",
    "        # 1) encoder의 인풋값을 ont-hot 인코딩(임베딩) 한다.\n",
    "        with tf.variable_scope('enc_embed_layer'):\n",
    "            encoder_oh = tf.one_hot(indices=self._enc_inp, depth=self._vocab_size) # encoder_oh: tensor, (batch_size, time steps, vocab_size)\n",
    "\n",
    "        #  2) encoder 셀을 쌓는다.\n",
    "        with tf.variable_scope('encoder'):\n",
    "            enc_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self._hidden_size, activation=tf.nn.tanh)\n",
    "            enc_cell = tf.nn.rnn_cell.MultiRNNCell([enc_cell] * self._num_layer)\n",
    "            _outputs, enc_states = tf.nn.dynamic_rnn(cell=enc_cell, inputs=encoder_oh, sequence_length=self._enc_seq_len, dtype=tf.float32)\n",
    "\n",
    "        # 3) 디코더의 임베딩을 원-핫인코딩으로 한다. tf.one_hot 함수를 사용해서 원-핫 인코딩을 해도 되지만. 나중에 GreedyEmbeddingsHelper를 사용하기 위해서는 임베딩 매트릭스를 만들어 주어야 한다.\n",
    "        with tf.variable_scope('dec_embed_layer'):\n",
    "            decoder_eye = tf.eye(num_rows=self._vocab_size)\n",
    "            decoder_embeddings = tf.get_variable(name='dec_embed', initializer=decoder_eye, trainable=False) # tf.nn.embedding_lookup 함수를 사용하려면 tf.get_variable을 params의 값으로\n",
    "\n",
    "            #decoder_embeddings = tf.get_variable(name='dec_embed', initializer=decoder_eye, trainable=False) # tf.nn.embedding_lookup 함수를 사용하려면 tf.get_variable을 params의 값으로\n",
    "            decoder_oh = tf.nn.embedding_lookup(params=decoder_embeddings, ids=self._dec_inp)    \n",
    "\n",
    "        # 4) decoder 셀을 쌓는다.    \n",
    "        with tf.variable_scope('decoder'):\n",
    "            \n",
    "            # decoder cell for both training and inference(학습과 추론과정은 같은 '가중치'를 공유함.)\n",
    "            dec_cell = tf.contrib.rnn.BasicLSTMCell(self._hidden_size, activation=tf.nn.tanh)\n",
    "            dec_cell = tf.nn.rnn_cell.MultiRNNCell([dec_cell] * self._num_layer)\n",
    "            \n",
    "            # Projection layer를 정의한다.\n",
    "            output_layer = tf.layers.Dense(vocab_size)\n",
    "\n",
    "            # 4-1) decoder의 training과 관련된 셀을 쌓는다.\n",
    "            with tf.variable_scope('training'):\n",
    "                train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_oh, sequence_length=self._dec_seq_len, time_major=False)\n",
    "                train_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=train_helper, initial_state=enc_states, output_layer=output_layer)\n",
    "                \n",
    "                # self._train_outputs.rnn_output: [batch_size x max(self._max_decoder_steps) x vocab_size]\n",
    "                # => 각 배치에서 가장 '긴' 값을 2번째 차원의 값으로 가진다는 점을 주의!! seq2seq2 loss 함수를 사용할 때 shape에 주의해야 한다.\n",
    "                # => bucketing과 연관이 있는 것 같은데.... 확실하지는 않다...\n",
    "                self._train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=train_decoder, impute_finished=True, output_time_major=False, \n",
    "                                                                                                                maximum_iterations=self._max_decoder_steps)\n",
    "                \n",
    "                # logits: [batch_size x max_dec_step x dec_vocab_size]\n",
    "                # => tf.identity: seq2seq 함수에서 logits에 들어갈 값 선언. tf.identity는 tensor의 contents와 shape를 똑같이 copy한다.\n",
    "                logits = tf.identity(self._train_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # btc_max_len: => logits의 shape에 맞춰주기 위함!!\n",
    "                # => decoder input \"배치\" 중에서  최대 sequence length(의미 있는 인풋이 들어오는 time step)\n",
    "                btc_max_len = tf.reduce_max(self._dec_seq_len, name='max_dec_len')\n",
    "                \n",
    "                # targets: \n",
    "                targets = tf.slice(input_=self._targets, begin=[0, 0], size=[-1, btc_max_len], name='targets')\n",
    "                              \n",
    "\n",
    "            # 4-2) decoder의 inference와 관련된 셀을 쌓는다.\n",
    "            with tf.variable_scope('inference'):\n",
    "                # 모델의 input을 dynamic shape로 하였으므로.. tf.shape(btc_enc_seq_len)로 해야 한다. btc_enc_seq_len.shape는 static shape일 때 사용 하는 것이다.\n",
    "                start_tokens = tf.fill(dims=tf.shape(btc_enc_seq_len), value=self._word2idx['<start>'])\n",
    "\n",
    "                infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=decoder_embeddings, \n",
    "                                                                                                              start_tokens=start_tokens, \n",
    "                                                                                                              end_token=self._word2idx['<end>'])\n",
    "                infer_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=infer_helper, initial_state=enc_states, output_layer=output_layer)\n",
    "                self._infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder, impute_finished=True, maximum_iterations=self._max_decoder_steps)\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_loss'):\n",
    "            # masksing: [batch_size x max_dec_len]\n",
    "            # => ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "            masking = tf.sequence_mask(lengths=self._dec_seq_len, maxlen=btc_max_len, dtype=tf.float32)\n",
    "                \n",
    "            # internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "            self.seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits=logits, targets=targets, weights=masking)\n",
    "            \n",
    "    def infer(self, sess,  btc_enc_seq_len, btc_enc_inp):\n",
    "        feed_infer = {self._enc_seq_len: btc_enc_seq_len, self._enc_inp: btc_enc_inp}\n",
    "        return sess.run(self._infer_outputs.sample_id, feed_dict=feed_infer)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 배치 관련 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?,), (?, ?), (?,), (?, ?), (?, ?)), types: (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32)>\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?,), dtype=int32) Tensor(\"IteratorGetNext:1\", shape=(?, ?), dtype=int32) Tensor(\"IteratorGetNext:2\", shape=(?,), dtype=int32) Tensor(\"IteratorGetNext:3\", shape=(?, ?), dtype=int32) Tensor(\"IteratorGetNext:4\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 인풋, 타겟, sequence_length와 관련된 placeholder를 정의한다.\n",
    "encoder_input = tf.placeholder(dtype=tf.int32, shape=[None, None]) # (batch size, time steps)\n",
    "decoder_input = tf.placeholder(tf.int32, [None, None]) # (batch size, time steps)\n",
    "targets = tf.placeholder(tf.int32, [None, None]) # (batch size, time steps)\n",
    "encoder_seq_len = tf.placeholder(tf.int32, [None]) # (batch size, ) 인코더 배치에 대응하는 sequence_length\n",
    "decoder_seq_len = tf.placeholder(tf.int32, [None]) # (batch size,) 디코더 배치에 대응하는 sequence_length\n",
    "\n",
    "# 배치 iterator를 정의하여 학습시 사용할 준비를 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((encoder_seq_len, encoder_input, decoder_seq_len, decoder_input, targets)) # 여러 플레이스 홀더를 데이터셋으로 하고\n",
    "dataset = dataset.shuffle(10000).batch(batch_size) # 랜덤으로 데이터셋을 섞은 다음에 배치크기는 미리 정의된 만큼 할 생각이야.\n",
    "iterator = dataset.make_initializable_iterator() # 위와 같은 사항을 반영하여 전체 데이터셋에서 배치 크기만큼 반복적으로 꺼내올 수 있는 iterator를 만들어 줘.\n",
    "btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets  = iterator.get_next() # 배치를 꺼내오는 명령문은 next_batch로 할게. (sess.run에서 실행 시켜야 다음 배치를 꺼내온다.)\n",
    "\n",
    "print(dataset)\n",
    "print(btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. object2text 클래스를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object2Text 객체의 argument들은 placeholder임을 명심하자.\n",
    "object2text = Object2Text(btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. optimizer를 정의하고 모델을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(lr).minimize(object2text.seq2seq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 4/4 [00:00<00:00, 11.31it/s, batch loss=2.45]\n",
      "Epoch 2/100: 100%|██████████| 4/4 [00:00<00:00, 102.88it/s, batch loss=2.04]\n",
      "Epoch 3/100: 100%|██████████| 4/4 [00:00<00:00, 91.79it/s, batch loss=1.19]\n",
      "Epoch 4/100: 100%|██████████| 4/4 [00:00<00:00, 83.74it/s, batch loss=0.945]\n",
      "Epoch 5/100: 100%|██████████| 4/4 [00:00<00:00, 78.53it/s, batch loss=0.98]\n",
      "Epoch 6/100: 100%|██████████| 4/4 [00:00<00:00, 79.33it/s, batch loss=0.715]\n",
      "Epoch 7/100: 100%|██████████| 4/4 [00:00<00:00, 76.15it/s, batch loss=0.38]\n",
      "Epoch 8/100: 100%|██████████| 4/4 [00:00<00:00, 75.63it/s, batch loss=0.602]\n",
      "Epoch 9/100: 100%|██████████| 4/4 [00:00<00:00, 76.93it/s, batch loss=0.431]\n",
      "Epoch 10/100: 100%|██████████| 4/4 [00:00<00:00, 74.51it/s, batch loss=0.476]\n",
      "Epoch 11/100: 100%|██████████| 4/4 [00:00<00:00, 71.04it/s, batch loss=0.395]\n",
      "Epoch 12/100: 100%|██████████| 4/4 [00:00<00:00, 75.78it/s, batch loss=0.315]\n",
      "Epoch 13/100: 100%|██████████| 4/4 [00:00<00:00, 83.68it/s, batch loss=0.467]\n",
      "Epoch 14/100: 100%|██████████| 4/4 [00:00<00:00, 83.16it/s, batch loss=0.295]\n",
      "Epoch 15/100: 100%|██████████| 4/4 [00:00<00:00, 83.33it/s, batch loss=0.0862]\n",
      "Epoch 16/100: 100%|██████████| 4/4 [00:00<00:00, 89.87it/s, batch loss=0.123]\n",
      "Epoch 17/100: 100%|██████████| 4/4 [00:00<00:00, 82.15it/s, batch loss=0.142]\n",
      "Epoch 18/100: 100%|██████████| 4/4 [00:00<00:00, 81.54it/s, batch loss=0.36]\n",
      "Epoch 19/100: 100%|██████████| 4/4 [00:00<00:00, 74.28it/s, batch loss=0.17]\n",
      "Epoch 20/100: 100%|██████████| 4/4 [00:00<00:00, 76.79it/s, batch loss=0.0312]\n",
      "Epoch 21/100: 100%|██████████| 4/4 [00:00<00:00, 81.15it/s, batch loss=0.323]\n",
      "Epoch 22/100: 100%|██████████| 4/4 [00:00<00:00, 69.41it/s, batch loss=0.197]\n",
      "Epoch 23/100: 100%|██████████| 4/4 [00:00<00:00, 79.35it/s, batch loss=0.0643]\n",
      "Epoch 24/100: 100%|██████████| 4/4 [00:00<00:00, 84.99it/s, batch loss=0.00856]\n",
      "Epoch 25/100: 100%|██████████| 4/4 [00:00<00:00, 86.17it/s, batch loss=0.0968]\n",
      "Epoch 26/100: 100%|██████████| 4/4 [00:00<00:00, 83.99it/s, batch loss=0.0939]\n",
      "Epoch 27/100: 100%|██████████| 4/4 [00:00<00:00, 85.18it/s, batch loss=0.0795]\n",
      "Epoch 28/100: 100%|██████████| 4/4 [00:00<00:00, 76.96it/s, batch loss=0.00775]\n",
      "Epoch 29/100: 100%|██████████| 4/4 [00:00<00:00, 89.94it/s, batch loss=0.0227]\n",
      "Epoch 30/100: 100%|██████████| 4/4 [00:00<00:00, 75.51it/s, batch loss=0.0485]\n",
      "Epoch 31/100: 100%|██████████| 4/4 [00:00<00:00, 74.06it/s, batch loss=0.0148]\n",
      "Epoch 32/100: 100%|██████████| 4/4 [00:00<00:00, 94.70it/s, batch loss=0.0344]\n",
      "Epoch 33/100: 100%|██████████| 4/4 [00:00<00:00, 89.27it/s, batch loss=0.034]\n",
      "Epoch 34/100: 100%|██████████| 4/4 [00:00<00:00, 89.84it/s, batch loss=0.00775]\n",
      "Epoch 35/100: 100%|██████████| 4/4 [00:00<00:00, 74.61it/s, batch loss=0.00879]\n",
      "Epoch 36/100: 100%|██████████| 4/4 [00:00<00:00, 73.01it/s, batch loss=0.00468]\n",
      "Epoch 37/100: 100%|██████████| 4/4 [00:00<00:00, 82.47it/s, batch loss=0.00868]\n",
      "Epoch 38/100: 100%|██████████| 4/4 [00:00<00:00, 78.43it/s, batch loss=0.00429]\n",
      "Epoch 39/100: 100%|██████████| 4/4 [00:00<00:00, 88.04it/s, batch loss=0.00337]\n",
      "Epoch 40/100: 100%|██████████| 4/4 [00:00<00:00, 87.61it/s, batch loss=0.00446]\n",
      "Epoch 41/100: 100%|██████████| 4/4 [00:00<00:00, 61.56it/s, batch loss=0.00291]\n",
      "Epoch 42/100: 100%|██████████| 4/4 [00:00<00:00, 76.75it/s, batch loss=0.00297]\n",
      "Epoch 43/100: 100%|██████████| 4/4 [00:00<00:00, 75.15it/s, batch loss=0.00281]\n",
      "Epoch 44/100: 100%|██████████| 4/4 [00:00<00:00, 85.06it/s, batch loss=0.00347]\n",
      "Epoch 45/100: 100%|██████████| 4/4 [00:00<00:00, 83.04it/s, batch loss=0.00277]\n",
      "Epoch 46/100: 100%|██████████| 4/4 [00:00<00:00, 92.63it/s, batch loss=0.00269]\n",
      "Epoch 47/100: 100%|██████████| 4/4 [00:00<00:00, 93.90it/s, batch loss=0.00346]\n",
      "Epoch 48/100: 100%|██████████| 4/4 [00:00<00:00, 96.12it/s, batch loss=0.00306]\n",
      "Epoch 49/100: 100%|██████████| 4/4 [00:00<00:00, 103.03it/s, batch loss=0.00272]\n",
      "Epoch 50/100: 100%|██████████| 4/4 [00:00<00:00, 95.12it/s, batch loss=0.00243]\n",
      "Epoch 51/100: 100%|██████████| 4/4 [00:00<00:00, 83.56it/s, batch loss=0.00219]\n",
      "Epoch 52/100: 100%|██████████| 4/4 [00:00<00:00, 87.25it/s, batch loss=0.00409]\n",
      "Epoch 53/100: 100%|██████████| 4/4 [00:00<00:00, 93.05it/s, batch loss=0.00317]\n",
      "Epoch 54/100: 100%|██████████| 4/4 [00:00<00:00, 100.65it/s, batch loss=0.0019]\n",
      "Epoch 55/100: 100%|██████████| 4/4 [00:00<00:00, 88.08it/s, batch loss=0.0019]\n",
      "Epoch 56/100: 100%|██████████| 4/4 [00:00<00:00, 87.56it/s, batch loss=0.00192]\n",
      "Epoch 57/100: 100%|██████████| 4/4 [00:00<00:00, 101.85it/s, batch loss=0.00158]\n",
      "Epoch 58/100: 100%|██████████| 4/4 [00:00<00:00, 95.60it/s, batch loss=0.00193]\n",
      "Epoch 59/100: 100%|██████████| 4/4 [00:00<00:00, 92.01it/s, batch loss=0.00167]\n",
      "Epoch 60/100: 100%|██████████| 4/4 [00:00<00:00, 88.26it/s, batch loss=0.00278]\n",
      "Epoch 61/100: 100%|██████████| 4/4 [00:00<00:00, 96.45it/s, batch loss=0.0013]\n",
      "Epoch 62/100: 100%|██████████| 4/4 [00:00<00:00, 98.69it/s, batch loss=0.00175]\n",
      "Epoch 63/100: 100%|██████████| 4/4 [00:00<00:00, 100.15it/s, batch loss=0.00146]\n",
      "Epoch 64/100: 100%|██████████| 4/4 [00:00<00:00, 91.26it/s, batch loss=0.0013]\n",
      "Epoch 65/100: 100%|██████████| 4/4 [00:00<00:00, 89.34it/s, batch loss=0.00155]\n",
      "Epoch 66/100: 100%|██████████| 4/4 [00:00<00:00, 114.35it/s, batch loss=0.00141]\n",
      "Epoch 67/100: 100%|██████████| 4/4 [00:00<00:00, 100.87it/s, batch loss=0.00214]\n",
      "Epoch 68/100: 100%|██████████| 4/4 [00:00<00:00, 106.04it/s, batch loss=0.00112]\n",
      "Epoch 69/100: 100%|██████████| 4/4 [00:00<00:00, 84.63it/s, batch loss=0.00134]\n",
      "Epoch 70/100: 100%|██████████| 4/4 [00:00<00:00, 87.82it/s, batch loss=0.00127]\n",
      "Epoch 71/100: 100%|██████████| 4/4 [00:00<00:00, 90.51it/s, batch loss=0.00107]\n",
      "Epoch 72/100: 100%|██████████| 4/4 [00:00<00:00, 95.37it/s, batch loss=0.00103]\n",
      "Epoch 73/100: 100%|██████████| 4/4 [00:00<00:00, 89.98it/s, batch loss=0.00119]\n",
      "Epoch 74/100: 100%|██████████| 4/4 [00:00<00:00, 90.30it/s, batch loss=0.0015]\n",
      "Epoch 75/100: 100%|██████████| 4/4 [00:00<00:00, 87.46it/s, batch loss=0.00166]\n",
      "Epoch 76/100: 100%|██████████| 4/4 [00:00<00:00, 81.68it/s, batch loss=0.0009]\n",
      "Epoch 77/100: 100%|██████████| 4/4 [00:00<00:00, 94.88it/s, batch loss=0.00133]\n",
      "Epoch 78/100: 100%|██████████| 4/4 [00:00<00:00, 99.96it/s, batch loss=0.000912]\n",
      "Epoch 79/100: 100%|██████████| 4/4 [00:00<00:00, 93.36it/s, batch loss=0.000947]\n",
      "Epoch 80/100: 100%|██████████| 4/4 [00:00<00:00, 81.76it/s, batch loss=0.000807]\n",
      "Epoch 81/100: 100%|██████████| 4/4 [00:00<00:00, 93.99it/s, batch loss=0.000892]\n",
      "Epoch 82/100: 100%|██████████| 4/4 [00:00<00:00, 107.01it/s, batch loss=0.000938]\n",
      "Epoch 83/100: 100%|██████████| 4/4 [00:00<00:00, 107.70it/s, batch loss=0.000885]\n",
      "Epoch 84/100: 100%|██████████| 4/4 [00:00<00:00, 94.88it/s, batch loss=0.000923]\n",
      "Epoch 85/100: 100%|██████████| 4/4 [00:00<00:00, 98.49it/s, batch loss=0.000788]\n",
      "Epoch 86/100: 100%|██████████| 4/4 [00:00<00:00, 89.97it/s, batch loss=0.00105]\n",
      "Epoch 87/100: 100%|██████████| 4/4 [00:00<00:00, 90.09it/s, batch loss=0.00131]\n",
      "Epoch 88/100: 100%|██████████| 4/4 [00:00<00:00, 98.95it/s, batch loss=0.000953]\n",
      "Epoch 89/100: 100%|██████████| 4/4 [00:00<00:00, 94.56it/s, batch loss=0.00122]\n",
      "Epoch 90/100: 100%|██████████| 4/4 [00:00<00:00, 92.04it/s, batch loss=0.000716]\n",
      "Epoch 91/100: 100%|██████████| 4/4 [00:00<00:00, 100.74it/s, batch loss=0.000768]\n",
      "Epoch 92/100: 100%|██████████| 4/4 [00:00<00:00, 92.91it/s, batch loss=0.000683]\n",
      "Epoch 93/100: 100%|██████████| 4/4 [00:00<00:00, 90.26it/s, batch loss=0.000777]\n",
      "Epoch 94/100: 100%|██████████| 4/4 [00:00<00:00, 97.32it/s, batch loss=0.000654]\n",
      "Epoch 95/100: 100%|██████████| 4/4 [00:00<00:00, 103.24it/s, batch loss=0.00106]\n",
      "Epoch 96/100: 100%|██████████| 4/4 [00:00<00:00, 100.78it/s, batch loss=0.000702]\n",
      "Epoch 97/100: 100%|██████████| 4/4 [00:00<00:00, 91.03it/s, batch loss=0.000873]\n",
      "Epoch 98/100: 100%|██████████| 4/4 [00:00<00:00, 95.49it/s, batch loss=0.00061]\n",
      "Epoch 99/100: 100%|██████████| 4/4 [00:00<00:00, 86.24it/s, batch loss=0.000652]\n",
      "Epoch 100/100: 100%|██████████| 4/4 [00:00<00:00, 87.83it/s, batch loss=0.000587]\n"
     ]
    }
   ],
   "source": [
    "num_batch = math.ceil(encoder_data.shape[0] / batch_size)\n",
    "\n",
    "sess = tf.Session() # 정의된 node와 operation들을 세션(그래프)에 할당한다.\n",
    "sess.run(tf.global_variables_initializer()) # 변수(가중치가 학습되는 노드)를 초기화 한다. (필수!!)\n",
    "\n",
    "epoc_loss_hist=[] # 에포크 당 loss를 저장하기 위한 리스트를 만든다.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # iterator.initalizer를 반복문 안에 넣어줘야 배치를 다 꺼내 쓴 다음에 다시 배치를 쓸 수 있다. \n",
    "    # 만약 아래 코드를 반복문 바깥에 두면 계속 OutOfRangeError 에러가 난다. (배치를 다 꺼내 썼는데 또 꺼내려고하면.. 당연히 에러!)\n",
    "    #  \n",
    "    sess.run(iterator.initializer, feed_dict={encoder_input: encoder_data, decoder_input: decoder_data,\n",
    "                                              targets: targets_data, encoder_seq_len: encoder_sequence_length, \n",
    "                                              decoder_seq_len: decoder_sequence_length})\n",
    "    \n",
    "    batch_loss_hist = []\n",
    "    \n",
    "    pbar = tqdm(range(num_batch)) # 배치 개수만큼 반복하는 progress bar 객체를 만든다. \n",
    "    \n",
    "    for i in pbar:\n",
    "\n",
    "        pbar.set_description(\"Epoch {}/{}\".format(epoch +1, epochs))\n",
    "        \n",
    "        _, loss = sess.run([train_op, object2text.seq2seq_loss])\n",
    "               \n",
    "        pbar.set_postfix({\"batch loss\": loss})\n",
    "        batch_loss_hist.append(loss)\n",
    "    \n",
    "    epoc_loss_hist.append(sum(batch_loss_hist) / num_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN Loss: 0.001 (100 epoch)\n"
     ]
    }
   ],
   "source": [
    "print(\"MIN Loss: {0:0.3f} ({1} epoch)\".format(min(epoc_loss_hist), epoc_loss_hist.index(min(epoc_loss_hist)) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 학습된 가중치가 저장된 sess를 통해서 inference를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = object2text.infer(sess, btc_enc_inp=encoder_data, btc_enc_seq_len=encoder_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  4 10  5  6 14  0]\n",
      " [11  2  1  6 14  0  0]\n",
      " [11  4 10  5 12 14  0]\n",
      " [11  2  1 12 14  0  0]\n",
      " [11  4 10  5  0  7 14]\n",
      " [11  2  1  0  7 14  0]\n",
      " [11  4 10  5  3  8 14]\n",
      " [11  2  1  3  8 14  0]\n",
      " [11  4 10  5  9 14  0]\n",
      " [11  2  1  9 14  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plane', 'take', 'off', 'from', 'dallas', '<end>', 'los'],\n",
       " ['plane', 'land', 'at', 'dallas', '<end>', 'los', 'los'],\n",
       " ['plane', 'take', 'off', 'from', 'chicago', '<end>', 'los'],\n",
       " ['plane', 'land', 'at', 'chicago', '<end>', 'los', 'los'],\n",
       " ['plane', 'take', 'off', 'from', 'los', 'angeles', '<end>'],\n",
       " ['plane', 'land', 'at', 'los', 'angeles', '<end>', 'los'],\n",
       " ['plane', 'take', 'off', 'from', 'new', 'york', '<end>'],\n",
       " ['plane', 'land', 'at', 'new', 'york', '<end>', 'los'],\n",
       " ['plane', 'take', 'off', 'from', 'miami', '<end>', 'los'],\n",
       " ['plane', 'land', 'at', 'miami', '<end>', 'los', 'los']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda row: [idx2word[idx] for idx in row], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pythonstudy.xyz/python/article/202-MySQL-%EC%BF%BC%EB%A6%AC\n",
    "\n",
    "pymysql 모듈을 이용하여 Mysql DB에서 데이터를 꺼내온다.\n",
    "\n",
    "- pymysql.connect() 메소드를 이용하여 MySQL DB에 connenct한다.\n",
    "- DB에 접속이 성공하면, cursor() 메소드를 호출하여 cursor객체를 가져온다. cursor() 메소드는 DB의 fetch동작을 관리한다. default는 fetchall()의 결과로 튜플을 반환하는데, DictCursor를 인자로 주면 fetchall()의 결과가 딕셔너리로 반환된다.\n",
    "- cursor객체에 execute() 메소드의 인자로 sql 문장을 사용하여 이를 DB 서버로 보낸다.\n",
    "- cursor객체에 fetchall(), fetchone(), fetchmany()등의 메소드를 이용하여 테이블에 있는 값을 가져온다.\n",
    "- connection 객체의 close() 메서드를 사용하여 DB 연결을 닫는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from_city': 'dallas', 'id': 1, 'to_city': 'los angeles', 'unit_cost': 2.0},\n",
      " {'from_city': 'dallas', 'id': 2, 'to_city': 'miami', 'unit_cost': 3.0},\n",
      " {'from_city': 'dallas', 'id': 3, 'to_city': 'new york', 'unit_cost': 10.0}]\n",
      "[{'from_city': 'chicago', 'id': 4, 'to_city': 'los angeles', 'unit_cost': 3.0},\n",
      " {'from_city': 'chicago', 'id': 5, 'to_city': 'miami', 'unit_cost': 2.0},\n",
      " {'from_city': 'chicago', 'id': 6, 'to_city': 'new york', 'unit_cost': 2.0}]\n",
      "[{'city_name': 'dallas', 'id': 1, 'x_quantity': 2000},\n",
      " {'city_name': 'chicago', 'id': 2, 'x_quantity': 2500}]\n",
      "[{'city_name': 'los angeles', 'id': 1, 'x_quantity': 3000},\n",
      " {'city_name': 'miami', 'id': 2, 'x_quantity': 2000},\n",
      " {'city_name': 'new york', 'id': 3, 'x_quantity': 2000}]\n"
     ]
    }
   ],
   "source": [
    "# MySQL connection을 연결\n",
    "conn = pymysql.connect(host='localhost', user='root', password='@@Qq9587410', db='transport', charset='utf8')\n",
    "\n",
    "# connection 객체를 통해서 cursor 객체를 생성.\n",
    "# conn.cursor() # array based cursor\n",
    "curs = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "# cursor 객체의 execute()의 인자로 sql 문을 전달하여 supply, demand, cost 테이블을 파이썬의 객체로 가져온다.\n",
    "curs.execute(\"select * from %s where from_city='%s'\" %('cost', 'dallas'))\n",
    "tlb_cost_dallas = curs.fetchall() # 데이터를 fetch.\n",
    "\n",
    "curs.execute(\"select * from %s where from_city='%s'\" %('cost', 'chicago'))\n",
    "tlb_cost_chicago = curs.fetchall() # 데이터를 fetch.\n",
    "\n",
    "curs.execute(\"select * from %s\" %('supply'))\n",
    "tlb_supply = curs.fetchall() # 데이터를 fetch.\n",
    "\n",
    "curs.execute(\"select * from %s\" %('demand'))\n",
    "tlb_demand = curs.fetchall() # 데이터를 fetch.\n",
    "\n",
    "conn.close() # 열린 connect 객체를 닫는다.\n",
    "\n",
    "pprint(tlb_cost_dallas)\n",
    "pprint(tlb_cost_chicago)\n",
    "pprint(tlb_supply)\n",
    "pprint(tlb_demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start of the formulation is a simple definition of the nodes and their limits/capacities. The node names are put into lists, and their associated capacities are put into dictionaries with the node names as the reference keys:\n",
    "\n",
    "https://www.coin-or.org/PuLP/CaseStudies/a_transportation_problem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dallas', 'chicago']\n",
      "{'dallas': 2000, 'chicago': 2500}\n",
      "['los angeles', 'miami', 'new york']\n",
      "{'los angeles': 3000, 'miami': 2000, 'new york': 2000}\n"
     ]
    }
   ],
   "source": [
    "# supply node의 이름을 리스트 형태로 만든다.\n",
    "sup_name = [row['city_name'] for row in tlb_supply]\n",
    "print(sup_name)\n",
    "\n",
    "# supply node의 공급량을 딕셔너리 형태로 정의한다.\n",
    "sup_qty = {row['city_name']: row['x_quantity'] for row in tlb_supply}\n",
    "print(sup_qty)\n",
    "\n",
    "# demand node의 이름을 리스트 형태로 만든다.\n",
    "dem_name = [row['city_name'] for row in tlb_demand]\n",
    "print(dem_name)\n",
    "\n",
    "# demand node의 수요량을 딕셔너리 형태로 정의한다.\n",
    "dem_qty = {row['city_name']: row['x_quantity'] for row in tlb_demand}\n",
    "print(dem_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dallas': {'los angeles': 2.0, 'miami': 3.0, 'new york': 10.0}, 'chicago': {'los angeles': 3.0, 'miami': 2.0, 'new york': 2.0}} \n",
      "\n",
      "Plane Distribution Problem:\n",
      "MINIMIZE\n",
      "None\n",
      "VARIABLES\n",
      " \n",
      "\n",
      "[('dallas', 'los angeles'), ('dallas', 'miami'), ('dallas', 'new york'), ('chicago', 'los angeles'), ('chicago', 'miami'), ('chicago', 'new york')] \n",
      "\n",
      "{'dallas': {'los angeles': route_dallas_los_angeles, 'miami': route_dallas_miami, 'new york': route_dallas_new_york}, 'chicago': {'los angeles': route_chicago_los_angeles, 'miami': route_chicago_miami, 'new york': route_chicago_new_york}}\n"
     ]
    }
   ],
   "source": [
    "# 각 경로의 cost가 정의된 리스트를 만든다.\n",
    "costs =[]\n",
    "costs.append(list(map(lambda row: row['unit_cost'], tlb_cost_dallas))) # dallas의 cost만으로 리스트를 만든다.\n",
    "costs.append(list(map(lambda row: row['unit_cost'], tlb_cost_chicago))) # chicago의 cost만으로 리스트를 만든다.\n",
    "costs = makeDict([sup_name, dem_name], costs) # pulp의 makeDict 모듈을 이용해서 딕셔너리 형태로 만든다.\n",
    "print(costs, \"\\n\")\n",
    "\n",
    "# 최소화 또는 최대화 문제를 풀수 있도록 틀을 짠다.\n",
    "# Creates the prob variable to contain the problem data(LpMinimize or LpMaximize)\n",
    "# 우리 문제는 비용의 '최소화(minimize)'가 objective이므로 LpMinimize로 한다.\n",
    "prob = LpProblem(name=\"Plane Distribution Problem\", sense=LpMinimize)\n",
    "print(prob, \"\\n\")\n",
    "\n",
    "# Creates a list of tuples containing all the possible routes for transport\n",
    "routes = [(s, d) for s in sup_name for d in dem_name]\n",
    "print(routes, \"\\n\")\n",
    "\n",
    "# A dictionary called route_vars is created to contain the referenced variables (the routes)\n",
    "# 각 경로에 흐르는 제품의 양(Xij)를 placeholder 느낌으로 정의해 주는 부분.\n",
    "# 제품의 양은 최소가 0이고, 최대는 무한대 그리고 정수임을 argument로 선언해 준다.\n",
    "route_vars = LpVariable.dicts(name=\"route\", indexs=(sup_name, dem_name), lowBound=0, upBound=None, cat=LpInteger) \n",
    "print(route_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plane Distribution Problem:\n",
      "MINIMIZE\n",
      "3.0*route_chicago_los_angeles + 2.0*route_chicago_miami + 2.0*route_chicago_new_york + 2.0*route_dallas_los_angeles + 3.0*route_dallas_miami + 10.0*route_dallas_new_york + 0.0\n",
      "SUBJECT TO\n",
      "_C1: route_dallas_los_angeles + route_dallas_miami + route_dallas_new_york\n",
      " <= 2000\n",
      "\n",
      "_C2: route_chicago_los_angeles + route_chicago_miami + route_chicago_new_york\n",
      " <= 2500\n",
      "\n",
      "_C3: route_chicago_los_angeles + route_dallas_los_angeles >= 3000\n",
      "\n",
      "_C4: route_chicago_miami + route_dallas_miami >= 2000\n",
      "\n",
      "_C5: route_chicago_new_york + route_dallas_new_york >= 2000\n",
      "\n",
      "VARIABLES\n",
      "0 <= route_chicago_los_angeles Integer\n",
      "0 <= route_chicago_miami Integer\n",
      "0 <= route_chicago_new_york Integer\n",
      "0 <= route_dallas_los_angeles Integer\n",
      "0 <= route_dallas_miami Integer\n",
      "0 <= route_dallas_new_york Integer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The objective function is added to prob first\n",
    "prob += lpSum([route_vars[s][d] * costs[s][d] for (s, d) in routes]) # \"Sum of Transporting Costs\"\n",
    "\n",
    "# The supply maximum constraints are added to prob for each supply node (warehouse)\n",
    "for s in sup_name:\n",
    "    prob += lpSum([route_vars[s][d] for d in dem_name]) <= sup_qty[s]\n",
    "    \n",
    "# The demand minimum constraints are added to prob for each demand node (bar)\n",
    "for d in dem_name:\n",
    "    prob += lpSum([route_vars[s][d] for s in sup_name]) >= dem_qty[d]\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 transportation model을 저장한다.\n",
    "prob.writeLP(\"transport model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = prob.solve()\n",
    "res_dic = {var: var.varValue for var in prob.variables()}\n",
    "result = pd.DataFrame({'routes': list(res_dic.keys()) + ['Total cost'], 'quantity': list(res_dic.values()) + [value(prob.objective)]})\n",
    "result.to_excel(\"/Users/ku/Desktop/transport.xlsx\", \"solution\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution을 엑셀 파일로 출력합니다.\n"
     ]
    }
   ],
   "source": [
    "# 저장된 엑셀 파일을 화면에 띄운다.\n",
    "import os\n",
    "print_exel_file = os.system(\"open -a '/Applications/Microsoft Excel.app' '/Users/ku/Desktop/transport.xlsx'\")\n",
    "if print_exel_file == 0: print(\"solution을 엑셀 파일로 출력합니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
