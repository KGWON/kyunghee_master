## 글을 이해하는데 필요한 기본 지식

- 벡터, 행렬의 개념 및 operation
- 선형대수학에서 등장하는 각종 notation



## 인공신경망과 선형대수학의 접점

#### 1. 인공신경망은 주어진 데이터가 모여있는 `manifold(또는 subspace)`를 찾는 알고리즘이다.

- `manifold`란 고차원의 공간에서 데이터들이 모여 있는 저차원의 부분공간(subspace)를 뜻한다.

- 머신러닝에서 주어진 데이터는 주로 over-determined system이다. 즉 변수의 개수보다 데이터의 개수가 많은 것이 일반적이다.

- Over-determined system에서는 최적 해를 찾을 수 없다. 주어진 데이터를 하나의 행렬(matrix)로 보았을 때, 각 변수들은 그 행렬의 열벡터들이라고 볼 수 있다. 열벡터들의 차원은 매우 높은것에 비해서 열공간을 span하는 열벡터들의 개수는 매우 적다. 따라서 매우 고차원의 공간에서 열벡터들의 span으로 형성된 부분공간은 열공간의 극히 일부분일 것이다. 그러므로 label 벡터가 부분공간에 포함될 확률은 매우 적다. 이 경우 최소제곱법(least square, 최소자승법)을 통해서 label 벡터를 근사하는 subspace에서의 하나의 벡터를 찾아야 한다.

- `manifold`또는 `subspace`


#### 2. 행렬의 곱을 column combination 관점에서 바라보면 input layer의 unit의 역할을 설명할 수 있다.



#### 3. 정규화(regularization) 기법은 무수히 많은 가중치 해중에서 가장 risk가 작은(일반화에 용이한) 해를 찾는 기법이다.

- 주어진 데이터 행렬의 열공간에 label vector가 포함 되어 있을 경우 열벡터가 선형독립인지 선형의존인지 여부에 따라서 최적 가중치 해의 개수가 달라진다.
- 열벡터가 선형독립이면 최적 가중치 해 개수가 단 하나 존재한다.
- 열벡터가 선형의존이면 최적 가중치 해 개수가 무수히 많이 존재한다. 다시말해 label vector를 찍는 평행사변형의 개수가 무수히 많다는 것이다.
- 크기가 큰 가중치가 선택되면 입력 데이터에 따라서 결과값이 크게 변동된다(=high variance low bias). 이는 주어진 데이터에서는 최적 해 중 하나이기 때문에 문제가 없지만 학습시 사용되지 않은 새로운 데이터를 모델이 인풋시켜면 예측력이 크게 떨어질 수 있다. 다시말해 모델의 일반화 문제가 발생하는 것이다(=overfitting)
- 크기가 작은 가중치가 선택되면 입력 데이터에 따라서도 결과값이 크게 변동되지는 않는다.(=low variance high bias) 학습시 사용되지 않는 데이터를 넣어도 일반화가 잘 될 수 있다. 그러나 오히려 모델의 예측력은 떨어질 수 있다.
- varinace, bias는 모두 *학습 데이터*에 대해서 설명되는 개념이다. 테스트 데이터라고 오해하지 말자!



#### 4. 다중공선성과 선형의존의 관계

**만일 다중공선성 문제가 있으면**

- VIF(분산팽창지수), 상관계수를 통해서 변수를 삭제하거나
- 선형의존(linearly dependent) 문제로 전환
  - 선형의존이면 역행렬이 존재하지 않음(non-invertible)
  - 역행렬이 존재하지 않으면
    - 해가 없다 ==> least square 문제로 전환. label vector가 projection된 subspace의 한 점으로 가는 경로가 다양하다.
    - 해가 무수히 많다. ==> 말 그대로 label vector로 향하는 경로가 다양하다
  - 두가지 경우 모두 궁극적으로는 다양한 해(다양한 가중치의 조합)가 존재한다.
  - 따라서 정규화(regularization) 기법을 통해서 중요하지 않은 변수의 가중치 크기를 줄인다.

