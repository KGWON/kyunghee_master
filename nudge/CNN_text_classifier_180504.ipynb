{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 문장분류기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 와이드 스크린으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wide display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 필요한 라이브러리를 불러온다.\n",
    "\n",
    "***tensorflow***: 핵심 딥러닝 프레임워크\n",
    "\n",
    "***numpy***: 수치 연산 핵심 프레임워크\n",
    "\n",
    "***pandas***: 데이터를 dataframe이라는 형식에 담아 편리하게 관리하기 위한 라이브러리\n",
    "\n",
    "***gensim***: Word2Vec 관련 라이브러리. 여기서는 pre-trained word vector를 불러오기 위해서 사용.\n",
    "\n",
    "***nltk***: tokenize와 관련된 라이브러리\n",
    "\n",
    "***pprint***: 출력 결과를 예쁘게 정렬 해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # for Word2Vec and Topic modeling\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize # for tokenizing\n",
    "from nltk.corpus import stopwords # for stopwords filtering\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 전처리 과정\n",
    "dt = pd.read_csv(\"/Users/ku/googleDrive/laboratory/개인 논문/nudge_sentence.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 'Sentence' 열만 따로 리스트에 저장하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using defaults in organ donation to increase compliance rates. Those countries where people are required to opt-out of organ donation report significantly higher consent than those with an opt-in policy. Possibly the most famous nudge, certainly the most eye-catching.', 'The authors sought to prime honesty by asking people to sign at the start of a form rather than the end when reporting how many miles they had driven on their car for insurance purposes. In this case there was a financial incentive to report less miles driven since reporting more would mean you would pay more (i.e. a higher number in the graph implies more honesty). The results indicated the treatment to be effective at inducing more honest declarations.', 'General Electric wanted its employees to stop smoking. They submitted to a Randomized Control Trial where the treatment group received cash incentives to quit. The control group received no incentives. Quitting for 6 months earned you $250, quitting for 12 months $400.', 'The _ave More Tomorrow [SMarT] program used defaults to increase employees savings rates by automatically increasing the % of their wage devoted to saving. Average saving rates for SMarT program participants increased from 3.5% to 13.6% over the course of 40 months. This is one of the most famous nudges.', 'The Behavioral Insights Team in the U.K. used social normative messages to increase tax compliance in 2011. The control group received standard tax letters. The treatment groups received the letters with an added normative messages. The difference in compliance rates between the control and the most effective treatment group was 15 percentage points.']\n"
     ]
    }
   ],
   "source": [
    "documents = list(dt['Sentence'])\n",
    "print(documents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize(정규표현식을 이용하여 문자와 숫자만 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The',\n",
      " 'authors',\n",
      " 'sought',\n",
      " 'prime',\n",
      " 'honesty',\n",
      " 'asking',\n",
      " 'people',\n",
      " 'sign',\n",
      " 'start',\n",
      " 'form',\n",
      " 'rather',\n",
      " 'end',\n",
      " 'reporting',\n",
      " 'many',\n",
      " 'miles',\n",
      " 'driven',\n",
      " 'car',\n",
      " 'insurance',\n",
      " 'purposes',\n",
      " 'In',\n",
      " 'case',\n",
      " 'financial',\n",
      " 'incentive',\n",
      " 'report',\n",
      " 'less',\n",
      " 'miles',\n",
      " 'driven',\n",
      " 'since',\n",
      " 'reporting',\n",
      " 'would',\n",
      " 'mean',\n",
      " 'would',\n",
      " 'pay',\n",
      " 'e',\n",
      " 'higher',\n",
      " 'number',\n",
      " 'graph',\n",
      " 'implies',\n",
      " 'honesty',\n",
      " 'The',\n",
      " 'results',\n",
      " 'indicated',\n",
      " 'treatment',\n",
      " 'effective',\n",
      " 'inducing',\n",
      " 'honest',\n",
      " 'declarations']\n"
     ]
    }
   ],
   "source": [
    "sentences = [regexp_tokenize(document, '[\\w]+') for document in documents]\n",
    "texts = [[word for word in sentence if word not in stopwords.words('english') ] for sentence in sentences]\n",
    "pprint(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3(option) 모델 학습을 위해 클래스를 임의로 할당한다. (절반은 0, 나머지는 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "클래스 '0': 51개\n",
      "클래스 '1': 50개\n"
     ]
    }
   ],
   "source": [
    "# 모델이 제대로 돌아가는지 확인 하기 위해 데이터의 반을 클래스 0으로 나머지 반을 클래스 1로 할당한다.\n",
    "dt.loc[:50, 'Class'] = 0\n",
    "dt.loc[50:, 'Class'] = 1\n",
    "print(\"\\n클래스 '0': \" + str(len(dt.loc[:50, 'Class'])) + \"개\")\n",
    "print(\"클래스 '1': \" + str(len(dt.loc[50:, 'Class'])) + \"개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 클래스 0을 [0, 1], 클래스 1을 [1,0]으로 바꿔서 변수 y에 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "print(y[0:10]): \n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 클래스를 0, 1 이 아니라 [0, 1], [0, 1]로 바꾸기.\n",
    "y = []\n",
    "for i in list(dt.loc[:, 'Class']):\n",
    "    if i == 0:\n",
    "        y.append([0, 1])\n",
    "    else:\n",
    "        y.append([1, 0])\n",
    "y = np.asarray(y)\n",
    "print(\"\\nprint(y[0:10]): \")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Embedding layer 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VocabularyProcessor**  \n",
    ": *'tf.nn.embedding_lookup'*의 ids로 활용하려면 단어에 '인덱스'가 할당 되어 있어야 하는데, 각 문서(문장)의 단어마다 **인덱스**를 할당해 주는 역할. 또한    문서마다 길이(단어의 개수)가 다를 수 있는데, *max_document_length*를 지정하여 문서의 길이를 같게 만들어 줌. (padding으로 0을 할당)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 2391\n",
      "CPU times: user 19.4 ms, sys: 965 µs, total: 20.4 ms\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# 셀 하나의 코드 수행시간을 출력 해 준다.\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length=200) # 객체 선언\n",
    "word_index = np.array(list(vocab_processor.fit_transform(documents)))\n",
    "\n",
    "# Extract word:id mapping from the object.\n",
    "# word to ix 와 유사\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "# Sort the vocabulary dictionary on the basis of values(id).\n",
    "sorted_vocab = sorted(vocab_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "# Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "# word with id i goes at index i of the list.\n",
    "vocabulary = list(list(zip(*sorted_vocab))[0])\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"vocab_size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lookup table을 만들기 위해서 pre-trained word embedding을 불러온다. \n",
    "# word_embedding = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/ku/Desktop/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input: word_index[:3, :]\n",
      "desired shape: [3, 2]\n",
      "result: \n",
      "[[ 4.24278831  4.50464582]\n",
      " [ 6.16410971  3.63599467]\n",
      " [ 2.95482111  7.94880581]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PARAMETERS:\n",
    "    - sequence_length: sentence의 길이(= 문장의 단어 개수, token의 개수)\n",
    "    - num_classes: class(label)의 개수\n",
    "    - vocab_size: 단어 사전의 총 단어수\n",
    "    - embedding_size: word vector의 차원\n",
    "    - filter_sizes: 필터의 크기 e.g. \"3,4,5\"\n",
    "    - num_filters: 필터의 개수, nonlinearity층의 차원 수와 관계 됨.\n",
    "    - L2_reg_lambda: L2 정규화 파라미터.\n",
    "    - filter_size: 필터의 크기 [3, 4, 5]\n",
    "'''\n",
    "\n",
    "sequence_length = word_index.shape[1]\n",
    "num_classes = 2\n",
    "batch_size = 16\n",
    "embedding_size = 300\n",
    "filter_size = [3, 4, 5]\n",
    "\n",
    "\n",
    "\"\"\" 중복 코드로서 필요없지만... 혹시 모르니껜 남겨두자.\n",
    "# embedding_layer를 만들기 위해 셋팅하자.\n",
    "W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "embedded_chars = tf.nn.embedding_lookup(W, input_x) # 차원수: [sequence length, embedding size]\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1) \n",
    "# 차원수 : [sequence length * embedding size * 1]\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph() # 기본 그래프를 초기화 한다.\n",
    "\n",
    "# input과 관련 된 Placeholder를 설정한다.\n",
    "orig_X = tf.placeholder(tf.int32, [None, sequence_length]) #tf.float32로 하면 tf.int32로 바꾸라며 에러가 난다.\n",
    "orig_Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# lookup table을 만든 후 위에서 만든 placeholder orig_X를 이용하여 임베딩 레이어를 생성한다.\n",
    "lookup = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0, seed=123)) # Making look-up table\n",
    "embedded_char = tf.nn.embedding_lookup(params=lookup, ids=orig_X)\n",
    "# tensorflow conv2d 함수의 요구 차원을 맞추기 위해서...! \n",
    "X = tf.expand_dims(embedded_char, -1) # dimension: [None, sequence_length, embedding_size, 1]\n",
    "\n",
    "# 필터를 만들어 준다.\n",
    "W1_1 = tf.Variable(tf.truncated_normal(shape=[filter_size[0], embedding_size, 1, 2])) \n",
    "W1_2 = tf.Variable(tf.truncated_normal(shape=[filter_size[1], embedding_size, 1, 2]))\n",
    "W1_3 = tf.Variable(tf.truncated_normal(shape=[filter_size[2], embedding_size, 1, 2]))\n",
    "# dimension: [filter_size, embedding_size, inpurt_channel, output_channel]\n",
    "\n",
    "b1_1 = tf.Variable(tf.constant(0.1), [2])\n",
    "b1_2 = tf.Variable(tf.constant(0.1), [2])\n",
    "b1_3 = tf.Variable(tf.constant(0.1), [2])\n",
    "# dimension: [output_channel]\n",
    "\n",
    "conv1_1 = tf.nn.relu(tf.nn.conv2d(X, W1_1, strides=[1, 1, 1, 1], padding=\"VALID\") + b1_1)\n",
    "conv1_2 = tf.nn.relu(tf.nn.conv2d(X, W1_2, strides=[1, 1, 1, 1], padding=\"VALID\") + b1_2)\n",
    "conv1_3 = tf.nn.relu(tf.nn.conv2d(X, W1_3, strides=[1, 1, 1, 1], padding=\"VALID\") + b1_3)\n",
    "# dimension: [None, sequence_length - filter_size + 1, 1, 2]\n",
    "\n",
    "max1_1 = tf.nn.max_pool(conv1_1, ksize=[1, sequence_length - 3 + 1, 1, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "max1_2 = tf.nn.max_pool(conv1_2, ksize=[1, sequence_length - 4 + 1, 1, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "max1_3 = tf.nn.max_pool(conv1_3, ksize=[1, sequence_length - 5 + 1, 1, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "# dimension: [None, 1, 1, 2]\n",
    "\n",
    "flatted_3 = tf.contrib.layers.flatten(max1_1)\n",
    "flatted_4 = tf.contrib.layers.flatten(max1_2)\n",
    "flatted_5 = tf.contrib.layers.flatten(max1_3)\n",
    "# dimension: [None, 2]\n",
    "\n",
    "dense = tf.concat([flatted_3, flatted_4, flatted_5], axis=1)\n",
    "W_dense = tf.Variable(tf.truncated_normal([6, num_classes], stddev=0.1, seed=123), name=\"W\")\n",
    "b = tf.Variable(tf.constant(0.1), [num_classes])\n",
    "Z = tf.matmul(dense, W_dense) + b\n",
    "# dimesion: [None, 2]\n",
    "\n",
    "print(\"\\ninput: word_index[:3, :]\" )\n",
    "print(\"desired shape: [3, 2]\")\n",
    "print(\"result: \")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(Z, feed_dict={orig_X: word_index[:3, :]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Loss, Back propagation, Weight update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 7.11484\n",
      "Epoch: 5, Cost: 0.337379\n",
      "Epoch: 10, Cost: 0.00141011\n",
      "Epoch: 15, Cost: 0.0071311\n",
      "Epoch: 20, Cost: 0.00172907\n",
      "Epoch: 25, Cost: 0.000538102\n",
      "\n",
      "final cost: 0.000314985 \n",
      "\n",
      "CPU times: user 39.6 s, sys: 3.15 s, total: 42.8 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prediction = tf.cast(tf.argmax(Z, 1), tf.float32) # [None, 1]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z, labels=orig_Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# epoch: 30\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(30):\n",
    "        _, cost = sess.run([optimizer, loss], feed_dict={orig_X: word_index, orig_Y: y})\n",
    "        if i % 5 == 0:\n",
    "            print(\"Epoch: \" + str(i) + \", Cost: \" + str(cost))\n",
    "\n",
    "print(\"\\nfinal cost:\", cost, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000314985\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
