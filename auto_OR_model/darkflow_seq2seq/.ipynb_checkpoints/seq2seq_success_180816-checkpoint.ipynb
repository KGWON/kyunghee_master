{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement seq2seq\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 모듈을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint # print() 함수의 결과를 아름답게(beautify) 보여주는 모듈\n",
    "from tools import * # 직접만든 함수 모음.\n",
    "from tqdm import tqdm # 배치학습 for문을 예쁘게 시각화 하는 모듈\n",
    "from tqdm import trange\n",
    "import math\n",
    "\n",
    "tf.reset_default_graph() # 텐서플로우 그래프를 초기화 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. corpus(YOLO의 output)를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([['dallas take off', ' plane take off from dallas'],\n",
      "       ['dallas land', ' plane land at Dallas'],\n",
      "       ['chicago take off', ' plane take off from chicago'],\n",
      "       ['chicago land', ' plane land at chicago'],\n",
      "       ['los angeles take off', ' plane take off from los angeles'],\n",
      "       ['los angeles land', ' plane land at los angeles'],\n",
      "       ['new york take off', ' plane take off from new york'],\n",
      "       ['new york land', ' plane land at new york'],\n",
      "       ['miami take off', ' plane take off from miami'],\n",
      "       ['miami land', ' plane land at miami']], dtype=object), <class 'numpy.ndarray'>)\n",
      "\n",
      "corpus's shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# pandas DataFrame을 numpy.ndarray로 바꾸려면 '.values'만 추가하면 된다.\n",
    "# => pd.read_table(...).values\n",
    "corpus = pd.read_table(\"./raw.csv\", delimiter=\",\").values\n",
    "\n",
    "print((corpus, type(corpus)))\n",
    "print(\"\\ncorpus's shape: {}\".format(corpus.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. lookup table를 생성하고 이를 이용하여 데이터를 인덱스로 맵핑한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq 모델의 cell은 인풋값으로 index로 된 numpy array나 리스트를 받는다.\n",
    "- tools.make_vocab(numpy.ndarray)\n",
    "    - numpy.ndarray: [batch size x time steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 17 \n",
      "\n",
      "idx2word: \n",
      " {0: 'land', 1: 'at', 2: 'plane', 3: 'york', 4: 'angeles', 5: 'los', 6: 'dallas', 7: 'chicago', 8: 'new', 9: 'off', 10: 'take', 11: 'from', 12: 'miami', 13: '<start>', 14: '<end>', 15: '<pad>', 16: '<unk>'} \n",
      "\n",
      "word2idx: \n",
      " {'land': 0, 'at': 1, 'plane': 2, 'york': 3, 'angeles': 4, 'los': 5, 'dallas': 6, 'chicago': 7, 'new': 8, 'off': 9, 'take': 10, 'from': 11, 'miami': 12, '<start>': 13, '<end>': 14, '<pad>': 15, '<unk>': 16} \n",
      "\n",
      "temp_encoder_data: \n",
      " [[6, 10, 9], [6, 0], [7, 10, 9], [7, 0], [5, 4, 10, 9], [5, 4, 0], [8, 3, 10, 9], [8, 3, 0], [12, 10, 9], [12, 0]] \n",
      "\n",
      "temp_decoder_data: \n",
      " [[2, 10, 9, 11, 6], [2, 0, 1, 6], [2, 10, 9, 11, 7], [2, 0, 1, 7], [2, 10, 9, 11, 5, 4], [2, 0, 1, 5, 4], [2, 10, 9, 11, 8, 3], [2, 0, 1, 8, 3], [2, 10, 9, 11, 12], [2, 0, 1, 12]] \n",
      "\n",
      "temp_targetsr_data: \n",
      " [[2, 10, 9, 11, 6], [2, 0, 1, 6], [2, 10, 9, 11, 7], [2, 0, 1, 7], [2, 10, 9, 11, 5, 4], [2, 0, 1, 5, 4], [2, 10, 9, 11, 8, 3], [2, 0, 1, 8, 3], [2, 10, 9, 11, 12], [2, 0, 1, 12]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make idx2word, word2idx, vocab_size\n",
    "idx2word, word2idx, vocab_size = make_vocab(corpus)\n",
    "\n",
    "# make_temp_data(numpy.ndarray, idx2word, word2idx)\n",
    "# => corpus를 인덱스 값으로 변환 시킨후 return한다. (<pad>, <start>, <end>, <unk> 토큰은 삽입되지 않음)\n",
    "# Dimension: batch size X time steps\n",
    "temp_encoder_data, temp_decoder_data, temp_targets_data = make_temp_data(corpus, idx2word=idx2word, word2idx=word2idx)\n",
    "\n",
    "# Identify temp_encoder_data, temp_decoder_data, temp_targets_data\n",
    "print('vocab_size: {} \\n'.format(vocab_size))\n",
    "print('idx2word: \\n {} \\n'.format(idx2word))\n",
    "print('word2idx: \\n {} \\n'.format(word2idx))\n",
    "\n",
    "# 패딩 토큰이 삽입되지 않아서 배치마다 time steps의 길이가 다르다. 이후에 수정 해 줄 것이다.\n",
    "print('temp_encoder_data: \\n {} \\n'.format(temp_encoder_data))\n",
    "print('temp_decoder_data: \\n {} \\n'.format(temp_decoder_data))\n",
    "print('temp_targetsr_data: \\n {} \\n'.format(temp_targets_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 각 데이터에 pad, end 토큰을 삽입하여 최종적인 데이터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tools.insert_tokens(numpy.ndarray, word2idx)\n",
    "# => insert_tokens()의 argument numpy.ndarray는 index로 변환 된 데이터를 받는다. 주의하자.\n",
    "# Dimension: batch size X time step\n",
    "encoder_data, decoder_data, targets_data, max_decoder_steps = insert_tokens(temp_encoder_data, temp_decoder_data, temp_targets_data, word2idx)\n",
    "\n",
    "# 최종적인 데이터가 잘 생성 되었는지를 확인하려면 아래 코드를 실행해보자.(옵션)\n",
    "# data_checker(encoder_data, idx2word)\n",
    "# data_checker(decoder_data, idx2word)\n",
    "# data_checker(targets_data, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. mask를 사용하기 위해서 sequence_length를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 3, 2, 4, 3, 4, 3, 3, 2]\n",
      "[6, 5, 6, 5, 7, 6, 7, 6, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "# 각 배치에서 <pad>를 제외한 실제 의미 있는 time steps의 길이를 구한다.\n",
    "# Dimension: batch size(1-D)\n",
    "encoder_sequence_length = sequence_length_maker(encoder_data, word2idx)\n",
    "decoder_sequence_length = sequence_length_maker(decoder_data, word2idx)\n",
    "\n",
    "print(encoder_sequence_length)\n",
    "print(decoder_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 하이퍼파라미터를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder의 outputs을 projection layer에 통과시키면 batch size X time step X class_size의 dimension을 얻게 된다.\n",
    "# 원-핫 인코딩이면 vocab_size와 같게 해 주어야 한다.\n",
    "class_size = vocab_size\n",
    "batch_size = 3\n",
    "num_batch = math.ceil(encoder_data.shape[0]/batch_size)\n",
    "lr = 0.01\n",
    "# seq2seq셀에 흐르는 hidden state의 유닛개수\n",
    "hidden_size = 128\n",
    "epochs = 100\n",
    "save_per_ckpt = 10\n",
    "# stacked RNN을 위한 하이퍼파라미터. 몇개의 레이어를 쌓을지 정해준다.\n",
    "num_layer = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델을 클래스로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object2Text:\n",
    "    # 'btc_': 배치\n",
    "    '''\n",
    "    Arguments:\n",
    "        btc_enc_seq_len: placeholder([None,]), 인코더 부분의 패딩을 제외한 실제 의미를 가지는 토큰의 개수를 배치별로 나타냄.\n",
    "        btc_enc_inp: placeholder([None, None]), 인코더의 인풋(batch size, time steps)\n",
    "        btc_dec_seq_len: placeholder([None,]), 디코더 부분의 패딩을 제외한 실제 의미를 가지는 토큰의 개수를 배치별로 나타냄.\n",
    "        btc_dec_inp: placeholder([Noen, None]), 디코더의 인풋(batch size, time steps)\n",
    "        btc_targets: placeholder([None, None]), groud truth(batch size, time steps)\n",
    "        word2idx: dictionary, '단어'를 키 값으로 조회하면 그에 해당하는 인덱스를 돌려주는 딕셔너리.\n",
    "        idx2word: dictionary, '인덱스'를 키 값으로 조회하면 그에 해당하는 인덱스를 돌려주는 딕셔너리.\n",
    "        vocab_size: scalar, 단어사전의 크기(단어사전에 있는 단어의 개수)\n",
    "        class_size: scalar, 디코더의 아웃풋을 fully connected 레이어와 연결하여 최종적으로 몇개의 클래스로 나타낼지 결정. 일반적으로 vocab size와 같게 하면 된다.\n",
    "        hidden_size: scalar, seq2seq 모델의 히든 유닛의 개수\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets, word2idx=word2idx, idx2word=idx2word, \n",
    "                     vocab_size=vocab_size, class_size=class_size, hidden_size=hidden_size, num_layer=num_layer, max_decoder_steps=max_decoder_steps):\n",
    "        \n",
    "        # 생성자의 인자들을 클래스멤버(?)로 만들어준다.\n",
    "        self._enc_seq_len = btc_enc_seq_len\n",
    "        self._enc_inp = btc_enc_inp\n",
    "        self._dec_seq_len = btc_dec_seq_len\n",
    "        self._dec_inp = btc_dec_inp\n",
    "        self._targets = btc_targets\n",
    "        self._word2idx = word2idx\n",
    "        self._idx2word = idx2word\n",
    "        self._vocab_size = vocab_size\n",
    "        self._class_size = class_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layer = num_layer\n",
    "        self._max_decoder_steps = max_decoder_steps\n",
    "    \n",
    "        # 1) encoder의 인풋값을 ont-hot 인코딩(임베딩) 한다.\n",
    "        with tf.variable_scope('enc_embed_layer'):\n",
    "            encoder_oh = tf.one_hot(indices=self._enc_inp, depth=self._vocab_size) # encoder_oh: tensor, (batch_size, time steps, vocab_size)\n",
    "\n",
    "        #  2) encoder 셀을 쌓는다.\n",
    "        with tf.variable_scope('encoder'):\n",
    "            enc_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self._hidden_size, activation=tf.nn.tanh)\n",
    "            enc_cell = tf.nn.rnn_cell.MultiRNNCell([enc_cell] * self._num_layer)\n",
    "            _outputs, enc_states = tf.nn.dynamic_rnn(cell=enc_cell, inputs=encoder_oh, sequence_length=self._enc_seq_len, dtype=tf.float32)\n",
    "\n",
    "        # 3) 디코더의 임베딩을 원-핫인코딩으로 한다. tf.one_hot 함수를 사용해서 원-핫 인코딩을 해도 되지만. 나중에 GreedyEmbeddingsHelper를 사용하기 위해서는 임베딩 매트릭스를 만들어 주어야 한다.\n",
    "        with tf.variable_scope('dec_embed_layer'):\n",
    "            decoder_eye = tf.eye(num_rows=self._vocab_size)\n",
    "            decoder_embeddings = tf.get_variable(name='dec_embed', initializer=decoder_eye, trainable=False) # tf.nn.embedding_lookup 함수를 사용하려면 tf.get_variable을 params의 값으로\n",
    "\n",
    "            #decoder_embeddings = tf.get_variable(name='dec_embed', initializer=decoder_eye, trainable=False) # tf.nn.embedding_lookup 함수를 사용하려면 tf.get_variable을 params의 값으로\n",
    "            decoder_oh = tf.nn.embedding_lookup(params=decoder_embeddings, ids=self._dec_inp)    \n",
    "\n",
    "        # 4) decoder 셀을 쌓는다.    \n",
    "        with tf.variable_scope('decoder'):\n",
    "            \n",
    "            # decoder cell for both training and inference(학습과 추론과정은 같은 '가중치'를 공유함.)\n",
    "            dec_cell = tf.contrib.rnn.BasicLSTMCell(self._hidden_size, activation=tf.nn.tanh)\n",
    "            dec_cell = tf.nn.rnn_cell.MultiRNNCell([dec_cell] * self._num_layer)\n",
    "            \n",
    "            # output projection (replacing `OutputProjectionWrapper`)\n",
    "            output_layer = tf.layers.Dense(vocab_size)\n",
    "#             score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell=dec_cell, output_size=self._class_size)\n",
    "\n",
    "            # 4-1) decoder의 training과 관련된 셀을 쌓는다.\n",
    "            with tf.variable_scope('training'):\n",
    "                train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_oh, sequence_length=self._dec_seq_len, time_major=False)\n",
    "                train_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=train_helper, initial_state=enc_states, output_layer=output_layer)\n",
    "                \n",
    "                # self._train_outputs.rnn_output: [batch_size x max(self._max_decoder_steps) x vocab_size]\n",
    "                # => 각 배치에서 가장 '긴' 값을 2번째 차원의 값으로 가진다는 점을 주의!! seq2seq2 loss 함수를 사용할 때 shape에 주의해야 한다.\n",
    "                # => bucketing과 연관이 있는 것 같은데.... 확실하지는 않다...\n",
    "                self._train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=train_decoder, impute_finished=True, output_time_major=False, \n",
    "                                                                                                                maximum_iterations=self._max_decoder_steps)\n",
    "                \n",
    "                # logits: [batch_size x max_dec_step x dec_vocab_size]\n",
    "                # => tf.identity: seq2seq 함수에서 logits에 들어갈 값 선언. tf.identity는 tensor의 contents와 shape를 똑같이 copy한다.\n",
    "                logits = tf.identity(self._train_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # btc_max_len: => logits의 shape에 맞춰주기 위함!!\n",
    "                # => decoder input \"배치\" 중에서  최대 sequence length(의미 있는 인풋이 들어오는 time step)\n",
    "                btc_max_len = tf.reduce_max(self._dec_seq_len, name='max_dec_len')\n",
    "                \n",
    "                # targets: \n",
    "                targets = tf.slice(input_=self._targets, begin=[0, 0], size=[-1, btc_max_len], name='targets')\n",
    "                              \n",
    "\n",
    "            # 4-2) decoder의 inference와 관련된 셀을 쌓는다.\n",
    "            with tf.variable_scope('inference'):\n",
    "                # 모델의 input을 dynamic shape로 하였으므로.. tf.shape(btc_enc_seq_len)로 해야 한다. btc_enc_seq_len.shape는 static shape일 때 사용 하는 것이다.\n",
    "                start_tokens = tf.fill(dims=tf.shape(btc_enc_seq_len), value=self._word2idx['<start>'])\n",
    "\n",
    "                infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=decoder_embeddings, \n",
    "                                                                                                              start_tokens=start_tokens, \n",
    "                                                                                                              end_token=self._word2idx['<end>'])\n",
    "                infer_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=infer_helper, initial_state=enc_states, output_layer=output_layer)\n",
    "                self._infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder, impute_finished=True, maximum_iterations=self._max_decoder_steps)\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_loss'):\n",
    "            #masksing: [batch_size x max_dec_len]\n",
    "            #=> ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "            masking = tf.sequence_mask(lengths=self._dec_seq_len, maxlen=btc_max_len, dtype=tf.float32)\n",
    "                \n",
    "            #internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "            self.seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits=logits, targets=targets, weights=masking)\n",
    "            \n",
    "    def infer(self, sess,  btc_enc_seq_len, btc_enc_inp):\n",
    "        feed_infer = {self._enc_seq_len: btc_enc_seq_len, self._enc_inp: btc_enc_inp}\n",
    "        return sess.run(self._infer_outputs.sample_id, feed_dict=feed_infer)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 배치 관련 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?,), (?, ?), (?,), (?, ?), (?, ?)), types: (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32)>\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?,), dtype=int32) Tensor(\"IteratorGetNext:1\", shape=(?, ?), dtype=int32) Tensor(\"IteratorGetNext:2\", shape=(?,), dtype=int32) Tensor(\"IteratorGetNext:3\", shape=(?, ?), dtype=int32) Tensor(\"IteratorGetNext:4\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## 인풋, 타겟, sequence_length와 관련된 placeholder를 정의한다.\n",
    "encoder_input = tf.placeholder(dtype=tf.int32, shape=[None, None]) # (batch size, time steps)\n",
    "decoder_input = tf.placeholder(tf.int32, [None, None]) # (batch size, time steps)\n",
    "targets = tf.placeholder(tf.int32, [None, None]) # (batch size, time steps)\n",
    "encoder_seq_len = tf.placeholder(tf.int32, [None]) # (batch size, ) 인코더 배치에 대응하는 sequence_length\n",
    "decoder_seq_len = tf.placeholder(tf.int32, [None]) # (batch size,) 디코더 배치에 대응하는 sequence_length\n",
    "\n",
    "## 배치 iterator를 정의하여 학습시 사용할 준비를 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((encoder_seq_len, encoder_input, decoder_seq_len, decoder_input, targets)) # 여러 플레이스 홀더를 데이터셋으로 하고\n",
    "dataset = dataset.shuffle(10000).batch(batch_size) # 랜덤으로 데이터셋을 섞은 다음에 배치크기는 미리 정의된 만큼 할 생각이야.\n",
    "iterator = dataset.make_initializable_iterator() # 위와 같은 사항을 반영하여 전체 데이터셋에서 배치 크기만큼 반복적으로 꺼내올 수 있는 iterator를 만들어 줘.\n",
    "btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets  = iterator.get_next() # 배치를 꺼내오는 명령문은 next_batch로 할게. (sess.run에서 실행 시켜야 다음 배치를 꺼내온다.)\n",
    "# next_element = iterator.get_next() # 배치를 꺼내오는 명령문은 next_batch로 할게. (sess.run에서 실행 시켜야 다음 배치를 꺼내온다.)\n",
    "\n",
    "print(dataset)\n",
    "# print(next_element)\n",
    "print(btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. object2text 클래스를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dask.dataframe' has no attribute 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7f40d17b8cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobject2text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObject2Text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbtc_enc_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbtc_enc_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbtc_dec_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbtc_dec_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbtc_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-f25f693c9d11>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets, word2idx, idx2word, vocab_size, class_size, hidden_size, num_layer, max_decoder_steps)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#  2) encoder 셀을 쌓는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0menc_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0menc_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menc_cell\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0m_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_oh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enc_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\distributions\\python\\ops\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlearn_io\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProblemType\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdnn_linear_combined\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_feeder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model_export_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_dask_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_dask_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHAS_DASK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\dask_io.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m   \u001b[0mallowed_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m   \u001b[0mHAS_DASK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dask.dataframe' has no attribute 'Series'"
     ]
    }
   ],
   "source": [
    "object2text = Object2Text(btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dask.dataframe' has no attribute 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-63834e902b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dask.dataframe' has no attribute 'Series'"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "dd.Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. optimizer를 정의하고 모델을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(lr).minimize(object2text.seq2seq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress:  25%|██▌       | 1/4 [00:00<00:01,  2.95it/s]\u001b[A\n",
      "Epoch Progress:   1%|          | 1/100 [00:00<00:36,  2.70it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 158.69it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 146.67it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 136.52it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:   5%|▌         | 5/100 [00:00<00:09, 10.31it/s]A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 169.30it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 159.79it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 148.39it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:   9%|▉         | 9/100 [00:00<00:06, 15.12it/s]A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 146.86it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 163.83it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 152.76it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  13%|█▎        | 13/100 [00:00<00:04, 18.31it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 140.15it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 145.25it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 159.45it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  17%|█▋        | 17/100 [00:00<00:04, 20.53it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 150.37it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 146.94it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 163.08it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  21%|██        | 21/100 [00:00<00:03, 22.24it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 143.61it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 146.23it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 164.82it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  25%|██▌       | 25/100 [00:01<00:03, 23.57it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 145.74it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 175.27it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 149.70it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  29%|██▉       | 29/100 [00:01<00:02, 24.66it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 153.34it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 151.97it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 151.92it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  33%|███▎      | 33/100 [00:01<00:02, 25.54it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 150.40it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 173.19it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 153.50it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  37%|███▋      | 37/100 [00:01<00:02, 26.38it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 150.22it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 157.40it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 158.70it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  41%|████      | 41/100 [00:01<00:02, 27.02it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 156.19it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 148.03it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 159.63it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  45%|████▌     | 45/100 [00:01<00:01, 27.55it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 146.24it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 123.52it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 149.51it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  49%|████▉     | 49/100 [00:01<00:01, 27.85it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 166.25it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 150.77it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 137.66it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  53%|█████▎    | 53/100 [00:01<00:01, 28.29it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 135.62it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 149.05it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 180.19it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  57%|█████▋    | 57/100 [00:01<00:01, 28.72it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 169.71it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 171.64it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 172.95it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  61%|██████    | 61/100 [00:02<00:01, 29.25it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 164.01it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 177.14it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 156.57it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  65%|██████▌   | 65/100 [00:02<00:01, 29.66it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 166.11it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 168.48it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 171.34it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  69%|██████▉   | 69/100 [00:02<00:01, 30.06it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 170.25it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 168.99it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 172.56it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  73%|███████▎  | 73/100 [00:02<00:00, 30.42it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 159.90it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 168.49it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 181.41it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  77%|███████▋  | 77/100 [00:02<00:00, 30.76it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 176.22it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 162.36it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 172.13it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  81%|████████  | 81/100 [00:02<00:00, 31.09it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 179.74it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 174.82it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 170.89it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  85%|████████▌ | 85/100 [00:02<00:00, 31.41it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 182.12it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 173.49it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 166.34it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  89%|████████▉ | 89/100 [00:02<00:00, 31.69it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 170.63it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 175.03it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 187.62it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  93%|█████████▎| 93/100 [00:02<00:00, 31.98it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 157.90it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 160.47it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 173.19it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress:  97%|█████████▋| 97/100 [00:03<00:00, 32.19it/s]\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 166.77it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Batch Proress: 100%|██████████| 4/4 [00:00<00:00, 173.43it/s]\u001b[A\n",
      "Batch Proress:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Progress: 100%|██████████| 100/100 [00:03<00:00, 32.34it/s]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "loss_hist=[]\n",
    "\n",
    "for i in trange(epochs, desc='Epoch Progress: '):\n",
    "\n",
    "    # iterator.initalizer를 반복문 안에 넣어줘야 배치를 다 꺼내 쓴 다음에 다시 배치를 쓸 수 있다. 이것을 반복문 바깥에 두면 계속 OutOfRangeError 에러가 난다.\n",
    "    sess.run(iterator.initializer, feed_dict={encoder_input: encoder_data, decoder_input: decoder_data,\n",
    "                                              targets: targets_data, encoder_seq_len: encoder_sequence_length, \n",
    "                                              decoder_seq_len: decoder_sequence_length})\n",
    "    \n",
    "    for j in trange(num_batch, desc=\"Batch Proress\"):\n",
    "        try:\n",
    "            _, loss = sess.run([train_op, object2text.seq2seq_loss])\n",
    "\n",
    "        # 에러가 발생한다는 것은 더 이상 꺼내 올 배치가 없다는 것이다. 따라서 배치를 꺼내오는 while문을 종료하고 다음 epoch로 넘어간다.\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "# from tqdm import trange\n",
    "# from time import sleep\n",
    "\n",
    "# for i in trange(10, desc='1st loop'):\n",
    "#     for j in trange(5, desc='2nd loop', leave=False):\n",
    "#         sleep(0.01)\n",
    "\n",
    "# t = trange(100, desc='seq2seq traing: ', leave=True, bar_format='{l_bar}{bar}{r_bar}')\n",
    "# for epoch in t:\n",
    "\n",
    "#     # iterator.initalizer를 반복문 안에 넣어줘야 배치를 다 꺼내 쓴 다음에 다시 배치를 쓸 수 있다. 이것을 반복문 바깥에 두면 계속 OutOfRangeError 에러가 난다.\n",
    "#     sess.run(iterator.initializer, feed_dict={encoder_input: encoder_data, decoder_input: decoder_data,\n",
    "#                                               targets: targets_data, encoder_seq_len: encoder_sequence_length, \n",
    "#                                               decoder_seq_len: decoder_sequence_length})\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             _, loss = sess.run([train_op, object2text.seq2seq_loss])\n",
    "\n",
    "#         # 에러가 발생한다는 것은 더 이상 꺼내 올 배치가 없다는 것이다. 따라서 배치를 꺼내오는 while문을 종료하고 다음 epoch로 넘어간다.\n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#             break\n",
    "    \n",
    "#     # Epoch당 loss를 나타내주는 tqdm 함수.\n",
    "#     t.set_postfix({\"Epoch loss\": loss})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 학습된 가중치가 저장된 sess를 통해서 inference를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = object2text.infer(sess, btc_enc_inp=encoder_data, btc_enc_seq_len=encoder_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  9  1  5 12 14  0]\n",
      " [ 3  8  0 12 14  0  0]\n",
      " [ 3  9  1  5  2 14  0]\n",
      " [ 3  8  0  2 14  0  0]\n",
      " [ 3  9  1  5  4  6 14]\n",
      " [ 3  8  0  4  6 14  0]\n",
      " [ 3  9  1  5 11  7 14]\n",
      " [ 3  8  0 11  7 14  0]\n",
      " [ 3  9  1  5 10 14  0]\n",
      " [ 3  8  0 10 14  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plane', 'take', 'off', 'from', 'dallas', '<end>', 'at'],\n",
       " ['plane', 'land', 'at', 'dallas', '<end>', 'at', 'at'],\n",
       " ['plane', 'take', 'off', 'from', 'chicago', '<end>', 'at'],\n",
       " ['plane', 'land', 'at', 'chicago', '<end>', 'at', 'at'],\n",
       " ['plane', 'take', 'off', 'from', 'los', 'angeles', '<end>'],\n",
       " ['plane', 'land', 'at', 'los', 'angeles', '<end>', 'at'],\n",
       " ['plane', 'take', 'off', 'from', 'new', 'york', '<end>'],\n",
       " ['plane', 'land', 'at', 'new', 'york', '<end>', 'at'],\n",
       " ['plane', 'take', 'off', 'from', 'miami', '<end>', 'at'],\n",
       " ['plane', 'land', 'at', 'miami', '<end>', 'at', 'at']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda row: [idx2word[idx] for idx in row], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import random\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    num_batch = 50\n",
    "    \n",
    "    pbar = tqdm_notebook(range(num_batch))\n",
    "    \n",
    "    for i in pbar:\n",
    "        avg_loss += random.randint(1,10)\n",
    "        pbar.set_description(\"Epoch {}/{} \".format(epoch+1, epochs))\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(avg_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
