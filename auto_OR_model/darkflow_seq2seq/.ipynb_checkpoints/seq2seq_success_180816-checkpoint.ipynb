{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement seq2seq\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 모듈을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caitech/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tools import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.reset_default_graph() # 텐서플로우 그래프를 초기화 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. corpus(YOLO의 output)를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([['dallas take off', ' plane take off from dallas'],\n",
      "       ['dallas land', ' plane land at Dallas'],\n",
      "       ['chicago take off', ' plane take off from chicago'],\n",
      "       ['chicago land', ' plane land at chicago'],\n",
      "       ['los angeles take off', ' plane take off from los angeles'],\n",
      "       ['los angeles land', ' plane land at los angeles'],\n",
      "       ['new york take off', ' plane take off from new york'],\n",
      "       ['new york land', ' plane land at new york'],\n",
      "       ['miami take off', ' plane take off from miami'],\n",
      "       ['miami land', ' plane land at miami']], dtype=object), <class 'numpy.ndarray'>)\n",
      "\n",
      "corpus's shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# pandas DataFrame을 numpy.ndarray로 바꾸려면 '.values'만 추가하면 된다.\n",
    "# => pd.read_table(...).values\n",
    "corpus = pd.read_table(\"./raw.csv\", delimiter=\",\").values\n",
    "\n",
    "print((corpus, type(corpus)))\n",
    "print(\"\\ncorpus's shape: {}\".format(corpus.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. lookup table를 생성하고 이를 이용하여 데이터를 인덱스로 맵핑한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq 모델의 cell은 인풋값으로 index로 된 numpy array나 리스트를 받는다.\n",
    "- tools.py의 make_vocab함수로 먼저 lookup table을 생성하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 17 \n",
      "\n",
      "idx2word: \n",
      " {0: 'from', 1: 'miami', 2: 'los', 3: 'chicago', 4: 'off', 5: 'take', 6: 'angeles', 7: 'plane', 8: 'new', 9: 'land', 10: 'dallas', 11: 'york', 12: 'at', 13: '<start>', 14: '<end>', 15: '<pad>', 16: '<unk>'} \n",
      "\n",
      "word2idx: \n",
      " {'from': 0, 'miami': 1, 'los': 2, 'chicago': 3, 'off': 4, 'take': 5, 'angeles': 6, 'plane': 7, 'new': 8, 'land': 9, 'dallas': 10, 'york': 11, 'at': 12, '<start>': 13, '<end>': 14, '<pad>': 15, '<unk>': 16} \n",
      "\n",
      "temp_encoder_data: \n",
      " [[10, 5, 4], [10, 9], [3, 5, 4], [3, 9], [2, 6, 5, 4], [2, 6, 9], [8, 11, 5, 4], [8, 11, 9], [1, 5, 4], [1, 9]] \n",
      "\n",
      "temp_decoder_data: \n",
      " [[7, 5, 4, 0, 10], [7, 9, 12, 10], [7, 5, 4, 0, 3], [7, 9, 12, 3], [7, 5, 4, 0, 2, 6], [7, 9, 12, 2, 6], [7, 5, 4, 0, 8, 11], [7, 9, 12, 8, 11], [7, 5, 4, 0, 1], [7, 9, 12, 1]] \n",
      "\n",
      "temp_targetsr_data: \n",
      " [[7, 5, 4, 0, 10], [7, 9, 12, 10], [7, 5, 4, 0, 3], [7, 9, 12, 3], [7, 5, 4, 0, 2, 6], [7, 9, 12, 2, 6], [7, 5, 4, 0, 8, 11], [7, 9, 12, 8, 11], [7, 5, 4, 0, 1], [7, 9, 12, 1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx2word, word2idx, vocab_size = make_vocab(corpus)\n",
    "\n",
    "temp_encoder_data, temp_decoder_data, temp_targets_data = make_temp_data(corpus,idx2word=idx2word, word2idx=word2idx)\n",
    "\n",
    "print('vocab_size: {} \\n'.format(vocab_size))\n",
    "print('idx2word: \\n {} \\n'.format(idx2word))\n",
    "print('word2idx: \\n {} \\n'.format(word2idx))\n",
    "\n",
    "print('temp_encoder_data: \\n {} \\n'.format(temp_encoder_data))\n",
    "print('temp_decoder_data: \\n {} \\n'.format(temp_decoder_data))\n",
    "print('temp_targetsr_data: \\n {} \\n'.format(temp_targets_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 각 데이터에 pad, end 토큰을 삽입하여 최종적인 데이터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dallas take off <pad>\n",
      "dallas land <pad> <pad>\n",
      "chicago take off <pad>\n",
      "chicago land <pad> <pad>\n",
      "los angeles take off\n",
      "los angeles land <pad>\n",
      "new york take off\n",
      "new york land <pad>\n",
      "miami take off <pad>\n",
      "miami land <pad> <pad>\n",
      "<start> plane take off from dallas <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane land at dallas <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane take off from chicago <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane land at chicago <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane take off from los angeles <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane land at los angeles <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane take off from new york <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane land at new york <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane take off from miami <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> plane land at miami <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane take off from dallas <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane land at dallas <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane take off from chicago <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane land at chicago <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane take off from los angeles <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane land at los angeles <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane take off from new york <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane land at new york <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane take off from miami <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "plane land at miami <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "encoder_data, decoder_data, targets_data, max_decoder_steps = insert_tokens(temp_encoder_data, temp_decoder_data, temp_targets_data, word2idx)\n",
    "\n",
    "data_checker(encoder_data, idx2word)\n",
    "data_checker(decoder_data, idx2word)\n",
    "data_checker(targets_data, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. mask를 사용하기 위해서 sequence_length를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 3, 2, 4, 3, 4, 3, 3, 2]\n",
      "[6, 5, 6, 5, 7, 6, 7, 6, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "encoder_sequence_length = sequence_length_maker(encoder_data, word2idx)\n",
    "decoder_sequence_length = sequence_length_maker(decoder_data, word2idx)\n",
    "print(encoder_sequence_length)\n",
    "print(decoder_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 하이퍼파라미터를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버퍼사이즈 레퍼런스\n",
    "https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. 하이퍼파라미터를 설정한다.\n",
    "class_size = vocab_size\n",
    "batch_size = 3\n",
    "lr = 0.01\n",
    "hidden_size = 128\n",
    "epochs = 100\n",
    "save_per_ckpt = 10\n",
    "num_layer = 1 # stacked RNN을 위한 하이퍼파라미터. 몇개의 레이어를 쌓을지 정해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델을 클래스로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seq2seq 모델을 만든다.\n",
    "\n",
    "class Object2Text:\n",
    "    '''\n",
    "    Arguments:\n",
    "        btc_enc_seq_len: placeholder([None,]), 인코더 부분의 패딩을 제외한 실제 의미를 가지는 토큰의 개수를 배치별로 나타냄.\n",
    "        btc_enc_inp: placeholder([None, None]), 인코더의 인풋(batch size, time steps)\n",
    "        btc_dec_seq_len: placeholder([None,]), 디코더 부분의 패딩을 제외한 실제 의미를 가지는 토큰의 개수를 배치별로 나타냄.\n",
    "        btc_dec_inp: placeholder([Noen, None]), 디코더의 인풋(batch size, time steps)\n",
    "        btc_targets: placeholder([None, None]), groud truth(batch size, time steps)\n",
    "        word2idx: dictionary, '단어'를 키 값으로 조회하면 그에 해당하는 인덱스를 돌려주는 딕셔너리.\n",
    "        idx2word: dictionary, '인덱스'를 키 값으로 조회하면 그에 해당하는 인덱스를 돌려주는 딕셔너리.\n",
    "        vocab_size: scalar, 단어사전의 크기(단어사전에 있는 단어의 개수)\n",
    "        class_size: scalar, 디코더의 아웃풋을 fully connected 레이어와 연결하여 최종적으로 몇개의 클래스로 나타낼지 결정. 일반적으로 vocab size와 같게 하면 된다.\n",
    "        hidden_size: scalar, seq2seq 모델의 히든 유닛의 개수\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets, word2idx=word2idx, idx2word=idx2word, \n",
    "                     vocab_size=vocab_size, class_size=class_size, hidden_size=hidden_size, num_layer=num_layer, max_decoder_steps=max_decoder_steps):\n",
    "        \n",
    "        # 생성자의 인자들을 클래스멤버(?)로 만들어준다.\n",
    "        self._enc_seq_len = btc_enc_seq_len\n",
    "        self._enc_inp = btc_enc_inp\n",
    "        self._dec_seq_len = btc_dec_seq_len\n",
    "        self._dec_inp = btc_dec_inp\n",
    "        self._targets = btc_targets\n",
    "        self._word2idx = word2idx\n",
    "        self._idx2word = idx2word\n",
    "        self._vocab_size = vocab_size\n",
    "        self._class_size = class_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layer = num_layer\n",
    "        self._max_decoder_steps = max_decoder_steps\n",
    "    \n",
    "        # 1) encoder의 인풋값을 ont-hot 인코딩(임베딩) 한다.\n",
    "        with tf.variable_scope('enc_embed_layer'):\n",
    "            encoder_oh = tf.one_hot(indices=self._enc_inp, depth=self._vocab_size) # encoder_oh: tensor, (batch_size, time steps, vocab_size)\n",
    "\n",
    "        #  2) encoder 셀을 쌓는다.\n",
    "        with tf.variable_scope('encoder'):\n",
    "            enc_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self._hidden_size, activation=tf.nn.tanh)\n",
    "            enc_cell = tf.nn.rnn_cell.MultiRNNCell([enc_cell] * self._num_layer)\n",
    "            _outputs, enc_states = tf.nn.dynamic_rnn(cell=enc_cell, inputs=encoder_oh, sequence_length=self._enc_seq_len, dtype=tf.float32)\n",
    "\n",
    "        # 3) 디코더의 임베딩을 원-핫인코딩으로 한다. tf.one_hot 함수를 사용해서 원-핫 인코딩을 해도 되지만. 나중에 GreedyEmbeddingsHelper를 사용하기 위해서는 임베딩 매트릭스를 만들어 주어야 한다.\n",
    "        with tf.variable_scope('dec_embed_layer'):\n",
    "            decoder_eye = tf.eye(num_rows=self._vocab_size)\n",
    "            decoder_embeddings = tf.get_variable(name='dec_embed', initializer=decoder_eye, trainable=False) # tf.nn.embedding_lookup 함수를 사용하려면 tf.get_variable을 params의 값으로\n",
    "\n",
    "            #decoder_embeddings = tf.get_variable(name='dec_embed', initializer=decoder_eye, trainable=False) # tf.nn.embedding_lookup 함수를 사용하려면 tf.get_variable을 params의 값으로\n",
    "            decoder_oh = tf.nn.embedding_lookup(params=decoder_embeddings, ids=self._dec_inp)    \n",
    "\n",
    "        # 4) decoder 셀을 쌓는다.    \n",
    "        with tf.variable_scope('decoder'):\n",
    "            \n",
    "            # decoder cell for both training and inference(학습과 추론과정은 같은 '가중치'를 공유함.)\n",
    "            dec_cell = tf.contrib.rnn.BasicLSTMCell(self._hidden_size, activation=tf.nn.tanh)\n",
    "            dec_cell = tf.nn.rnn_cell.MultiRNNCell([dec_cell] * self._num_layer)\n",
    "            \n",
    "            # output projection (replacing `OutputProjectionWrapper`)\n",
    "            output_layer = tf.layers.Dense(vocab_size)\n",
    "#             score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell=dec_cell, output_size=self._class_size)\n",
    "\n",
    "            # 4-1) decoder의 training과 관련된 셀을 쌓는다.\n",
    "            with tf.variable_scope('training'):\n",
    "                train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_oh, sequence_length=self._dec_seq_len, time_major=False)\n",
    "                train_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=train_helper, initial_state=enc_states, output_layer=output_layer)\n",
    "                \n",
    "                # self._train_outputs.rnn_output: [batch_size x max(self._max_decoder_steps) x vocab_size]\n",
    "                # => 각 배치에서 가장 '긴' 값을 2번째 차원의 값으로 가진다는 점을 주의!! seq2seq2 loss 함수를 사용할 때 shape에 주의해야 한다.\n",
    "                # => bucketing과 연관이 있는 것 같은데.... 확실하지는 않다...\n",
    "                self._train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=train_decoder, impute_finished=True, output_time_major=False, \n",
    "                                                                                                                maximum_iterations=self._max_decoder_steps)\n",
    "                \n",
    "                # logits: [batch_size x max_dec_step x dec_vocab_size]\n",
    "                # => tf.identity: seq2seq 함수에서 logits에 들어갈 값 선언. tf.identity는 tensor의 contents와 shape를 똑같이 copy한다.\n",
    "                logits = tf.identity(self._train_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # btc_max_len: => logits의 shape에 맞춰주기 위함!!\n",
    "                # => decoder input \"배치\" 중에서  최대 sequence length(의미 있는 인풋이 들어오는 time step)\n",
    "                btc_max_len = tf.reduce_max(self._dec_seq_len, name='max_dec_len')\n",
    "                \n",
    "                # targets: \n",
    "                targets = tf.slice(input_=self._targets, begin=[0, 0], size=[-1, btc_max_len], name='targets')\n",
    "                              \n",
    "\n",
    "            # 4-2) decoder의 inference와 관련된 셀을 쌓는다.\n",
    "            with tf.variable_scope('inference'):\n",
    "                # 모델의 input을 dynamic shape로 하였으므로.. tf.shape(btc_enc_seq_len)로 해야 한다. btc_enc_seq_len.shape는 static shape일 때 사용 하는 것이다.\n",
    "                start_tokens = tf.fill(dims=tf.shape(btc_enc_seq_len), value=self._word2idx['<start>'])\n",
    "\n",
    "                infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=decoder_embeddings, \n",
    "                                                                                                              start_tokens=start_tokens, \n",
    "                                                                                                              end_token=self._word2idx['<end>'])\n",
    "                infer_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=infer_helper, initial_state=enc_states, output_layer=output_layer)\n",
    "                self._infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder, impute_finished=True, maximum_iterations=self._max_decoder_steps)\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_loss'):\n",
    "            #masksing: [batch_size x max_dec_len]\n",
    "            #=> ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "            masking = tf.sequence_mask(lengths=self._dec_seq_len, maxlen=btc_max_len, dtype=tf.float32)\n",
    "                \n",
    "            #internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "            self.seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits=logits, targets=targets, weights=masking)\n",
    "            \n",
    "    def infer(self, sess,  btc_enc_seq_len, btc_enc_inp):\n",
    "        feed_infer = {self._enc_seq_len: btc_enc_seq_len, self._enc_inp: btc_enc_inp}\n",
    "        return sess.run(self._infer_outputs.sample_id, feed_dict=feed_infer)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 배치 관련 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?,), (?, ?), (?,), (?, ?), (?, ?)), types: (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32)>\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?,), dtype=int32) Tensor(\"IteratorGetNext:1\", shape=(?, ?), dtype=int32) Tensor(\"IteratorGetNext:2\", shape=(?,), dtype=int32) Tensor(\"IteratorGetNext:3\", shape=(?, ?), dtype=int32) Tensor(\"IteratorGetNext:4\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## 인풋, 타겟, sequence_length와 관련된 placeholder를 정의한다.\n",
    "encoder_input = tf.placeholder(dtype=tf.int32, shape=[None, None]) # (batch size, time steps)\n",
    "decoder_input = tf.placeholder(tf.int32, [None, None]) # (batch size, time steps)\n",
    "targets = tf.placeholder(tf.int32, [None, None]) # (batch size, time steps)\n",
    "encoder_seq_len = tf.placeholder(tf.int32, [None]) # (batch size, ) 인코더 배치에 대응하는 sequence_length\n",
    "decoder_seq_len = tf.placeholder(tf.int32, [None]) # (batch size,) 디코더 배치에 대응하는 sequence_length\n",
    "\n",
    "## 배치 iterator를 정의하여 학습시 사용할 준비를 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((encoder_seq_len, encoder_input, decoder_seq_len, decoder_input, targets)) # 여러 플레이스 홀더를 데이터셋으로 하고\n",
    "dataset = dataset.shuffle(10000).batch(batch_size) # 랜덤으로 데이터셋을 섞은 다음에 배치크기는 미리 정의된 만큼 할 생각이야.\n",
    "iterator = dataset.make_initializable_iterator() # 위와 같은 사항을 반영하여 전체 데이터셋에서 배치 크기만큼 반복적으로 꺼내올 수 있는 iterator를 만들어 줘.\n",
    "btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets  = iterator.get_next() # 배치를 꺼내오는 명령문은 next_batch로 할게. (sess.run에서 실행 시켜야 다음 배치를 꺼내온다.)\n",
    "# next_element = iterator.get_next() # 배치를 꺼내오는 명령문은 next_batch로 할게. (sess.run에서 실행 시켜야 다음 배치를 꺼내온다.)\n",
    "\n",
    "print(dataset)\n",
    "# print(next_element)\n",
    "print(btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. object2text 클래스를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "object2text = Object2Text(btc_enc_seq_len, btc_enc_inp, btc_dec_seq_len, btc_dec_inp, btc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. optimizer를 정의하고 모델을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(lr).minimize(object2text.seq2seq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:07, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8287358\n",
      "2.7516203\n",
      "2.631255\n",
      "2.3359888\n",
      "Epoch: 1\n",
      "2.556924\n",
      "2.2986786\n",
      "2.195085\n",
      "2.2393222\n",
      "Epoch: 2\n",
      "2.0021868\n",
      "1.8802986\n",
      "1.7426249\n",
      "1.5553432\n",
      "Epoch: 3\n",
      "1.4535738\n",
      "1.3574306\n",
      "1.3032799\n",
      "0.77499694\n",
      "Epoch: 4\n",
      "1.1055855\n",
      "1.0190586\n",
      "0.97734785\n",
      "0.6990683\n",
      "Epoch: 5\n",
      "0.76777536\n",
      "0.76025045\n",
      "0.658766\n",
      "0.7378437\n",
      "Epoch: 6\n",
      "0.5551729\n",
      "0.5099685\n",
      "0.7153764\n",
      "0.69321495\n",
      "Epoch: 7\n",
      "0.50605214\n",
      "0.480077\n",
      "0.5381617\n",
      "0.5194366\n",
      "Epoch: 8\n",
      "0.42977202\n",
      "0.41830173\n",
      "0.59515595\n",
      "0.44226918\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:03, 22.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42403686\n",
      "0.4038578\n",
      "0.43658376\n",
      "0.6687717\n",
      "Epoch: 10\n",
      "0.34663966\n",
      "0.38252452\n",
      "0.3915381\n",
      "0.4263312\n",
      "Epoch: 11\n",
      "0.316421\n",
      "0.2568592\n",
      "0.6681287\n",
      "0.18608673\n",
      "Epoch: 12\n",
      "0.19729348\n",
      "0.45875627\n",
      "0.57823706\n",
      "0.28871176\n",
      "Epoch: 13\n",
      "0.62096167\n",
      "0.49683625\n",
      "0.51478046\n",
      "0.37151578\n",
      "Epoch: 14\n",
      "0.3465386\n",
      "0.47813696\n",
      "0.21743281\n",
      "0.42112654\n",
      "Epoch: 15\n",
      "0.36813244\n",
      "0.29108444\n",
      "0.42284098\n",
      "0.35761762\n",
      "Epoch: 16\n",
      "0.187106\n",
      "0.38087276\n",
      "0.41873863\n",
      "0.19191267\n",
      "Epoch: 17\n",
      "0.24999762\n",
      "0.22457254\n",
      "0.20881052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:00<00:02, 27.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2848507\n",
      "Epoch: 18\n",
      "0.35866404\n",
      "0.23149855\n",
      "0.22189753\n",
      "0.2128921\n",
      "Epoch: 19\n",
      "0.24023074\n",
      "0.26948193\n",
      "0.17607561\n",
      "0.23846476\n",
      "Epoch: 20\n",
      "0.12539044\n",
      "0.20151538\n",
      "0.32427582\n",
      "0.27775478\n",
      "Epoch: 21\n",
      "0.26876894\n",
      "0.18278275\n",
      "0.183074\n",
      "0.07366007\n",
      "Epoch: 22\n",
      "0.23224223\n",
      "0.17085813\n",
      "0.22005801\n",
      "0.05551127\n",
      "Epoch: 23\n",
      "0.20019293\n",
      "0.1642757\n",
      "0.13365202\n",
      "0.25681368\n",
      "Epoch: 24\n",
      "0.15577634\n",
      "0.17886391\n",
      "0.12368272\n",
      "0.21644475\n",
      "Epoch: 25\n",
      "0.1399243\n",
      "0.17479256\n",
      "0.16690874\n",
      "0.048155528\n",
      "Epoch: 26\n",
      "0.10257149\n",
      "0.18302271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 30/100 [00:01<00:02, 28.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14944999\n",
      "0.22677255\n",
      "Epoch: 27\n",
      "0.13210334\n",
      "0.111124106\n",
      "0.11670966\n",
      "0.25774926\n",
      "Epoch: 28\n",
      "0.13763176\n",
      "0.09850278\n",
      "0.10574736\n",
      "0.20874123\n",
      "Epoch: 29\n",
      "0.13045847\n",
      "0.09510035\n",
      "0.12716919\n",
      "0.030218037\n",
      "Epoch: 30\n",
      "0.06120705\n",
      "0.15741935\n",
      "0.07794413\n",
      "0.18257736\n",
      "Epoch: 31\n",
      "0.07797617\n",
      "0.06816676\n",
      "0.07117749\n",
      "0.26424533\n",
      "Epoch: 32\n",
      "0.07550228\n",
      "0.11513644\n",
      "0.08472042\n",
      "0.06747593\n",
      "Epoch: 33\n",
      "0.050046712\n",
      "0.095546134\n",
      "0.095598444\n",
      "0.0604131\n",
      "Epoch: 34\n",
      "0.089506954\n",
      "0.041752953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:01<00:01, 31.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07959538\n",
      "0.041222114\n",
      "Epoch: 35\n",
      "0.05331724\n",
      "0.04441551\n",
      "0.04167757\n",
      "0.16456932\n",
      "Epoch: 36\n",
      "0.06455707\n",
      "0.04911085\n",
      "0.07072224\n",
      "0.08474433\n",
      "Epoch: 37\n",
      "0.029895421\n",
      "0.0446558\n",
      "0.043611474\n",
      "0.030690534\n",
      "Epoch: 38\n",
      "0.02944007\n",
      "0.020784074\n",
      "0.04488379\n",
      "0.09007875\n",
      "Epoch: 39\n",
      "0.02533942\n",
      "0.03362296\n",
      "0.02995242\n",
      "0.01247976\n",
      "Epoch: 40\n",
      "0.045194432\n",
      "0.025956446\n",
      "0.010393914\n",
      "0.013821989\n",
      "Epoch: 41\n",
      "0.03384864\n",
      "0.017302558\n",
      "0.019438393\n",
      "0.0063051246\n",
      "Epoch: 42\n",
      "0.010488902\n",
      "0.028646722\n",
      "0.012630569\n",
      "0.0061832443\n",
      "Epoch: 43\n",
      "0.016443163\n",
      "0.016541824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:01<00:01, 33.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00814902\n",
      "0.005354829\n",
      "Epoch: 44\n",
      "0.013024297\n",
      "0.0053061657\n",
      "0.015700981\n",
      "0.0054743965\n",
      "Epoch: 45\n",
      "0.0053080884\n",
      "0.0114827845\n",
      "0.009386324\n",
      "0.013578108\n",
      "Epoch: 46\n",
      "0.00892243\n",
      "0.006726102\n",
      "0.008627025\n",
      "0.0076013897\n",
      "Epoch: 47\n",
      "0.006066198\n",
      "0.009734444\n",
      "0.007012569\n",
      "0.0040003387\n",
      "Epoch: 48\n",
      "0.006644384\n",
      "0.008244175\n",
      "0.0057823053\n",
      "0.0039236466\n",
      "Epoch: 49\n",
      "0.004239677\n",
      "0.005713744\n",
      "0.007982444\n",
      "0.005772615\n",
      "Epoch: 50\n",
      "0.005500239\n",
      "0.005088658\n",
      "0.0064378\n",
      "0.0033101146\n",
      "Epoch: 51\n",
      "0.004119385\n",
      "0.0054044025\n",
      "0.005408106\n",
      "0.0050798627\n",
      "Epoch: 52\n",
      "0.0052416255\n",
      "0.004176321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:01<00:01, 34.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0044793044\n",
      "0.0043159435\n",
      "Epoch: 53\n",
      "0.0039029543\n",
      "0.004591328\n",
      "0.0038650644\n",
      "0.0057560825\n",
      "Epoch: 54\n",
      "0.0036821647\n",
      "0.003815827\n",
      "0.0044683665\n",
      "0.0035537966\n",
      "Epoch: 55\n",
      "0.003643276\n",
      "0.0035909335\n",
      "0.0040683206\n",
      "0.003248217\n",
      "Epoch: 56\n",
      "0.004189459\n",
      "0.0032509249\n",
      "0.0033516916\n",
      "0.002495856\n",
      "Epoch: 57\n",
      "0.0033084233\n",
      "0.0040666955\n",
      "0.0027117142\n",
      "0.0031206785\n",
      "Epoch: 58\n",
      "0.0038248033\n",
      "0.0030100653\n",
      "0.002952739\n",
      "0.0022040566\n",
      "Epoch: 59\n",
      "0.00311847\n",
      "0.003236378\n",
      "0.0029074997\n",
      "0.0020631314\n",
      "Epoch: 60\n",
      "0.0030459124\n",
      "0.0027086786\n",
      "0.002885012\n",
      "0.0027726826\n",
      "Epoch: 61\n",
      "0.003129304\n",
      "0.002900998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:01<00:00, 35.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0022778264\n",
      "0.002354605\n",
      "Epoch: 62\n",
      "0.0026934142\n",
      "0.0025573517\n",
      "0.0026410192\n",
      "0.0026008359\n",
      "Epoch: 63\n",
      "0.0030166963\n",
      "0.0023605907\n",
      "0.0023838815\n",
      "0.002118259\n",
      "Epoch: 64\n",
      "0.002281969\n",
      "0.002557665\n",
      "0.0026426371\n",
      "0.0019178161\n",
      "Epoch: 65\n",
      "0.002768983\n",
      "0.0021363313\n",
      "0.0020725485\n",
      "0.002472205\n",
      "Epoch: 66\n",
      "0.002317327\n",
      "0.0025353425\n",
      "0.0020529823\n",
      "0.0017603824\n",
      "Epoch: 67\n",
      "0.0021723928\n",
      "0.0023544107\n",
      "0.0019493182\n",
      "0.0020927957\n",
      "Epoch: 68\n",
      "0.0021813158\n",
      "0.002132544\n",
      "0.0019685132\n",
      "0.0018652102\n",
      "Epoch: 69\n",
      "0.0017901888\n",
      "0.002385126\n",
      "0.0020147695\n",
      "0.0013986873\n",
      "Epoch: 70\n",
      "0.0017821871\n",
      "0.0019204421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 75/100 [00:02<00:00, 36.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019301283\n",
      "0.0024992477\n",
      "Epoch: 71\n",
      "0.001963091\n",
      "0.0016482725\n",
      "0.0020442165\n",
      "0.0019026482\n",
      "Epoch: 72\n",
      "0.0017578406\n",
      "0.0019604212\n",
      "0.0017841207\n",
      "0.0016299753\n",
      "Epoch: 73\n",
      "0.0015646787\n",
      "0.0020843023\n",
      "0.0016476922\n",
      "0.0017216158\n",
      "Epoch: 74\n",
      "0.0016044154\n",
      "0.0017281264\n",
      "0.0017958866\n",
      "0.0016504709\n",
      "Epoch: 75\n",
      "0.0018224275\n",
      "0.0014435166\n",
      "0.0016891354\n",
      "0.0015897342\n",
      "Epoch: 76\n",
      "0.0015679478\n",
      "0.0015597207\n",
      "0.0016285279\n",
      "0.0018291175\n",
      "Epoch: 77\n",
      "0.001427059\n",
      "0.001726993\n",
      "0.001489184\n",
      "0.0017365104\n",
      "Epoch: 78\n",
      "0.0015316406\n",
      "0.0015874123\n",
      "0.0014067913\n",
      "0.0016228454\n",
      "Epoch: 79\n",
      "0.0015975502\n",
      "0.0013627225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:02<00:00, 37.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001608737\n",
      "0.0010912016\n",
      "Epoch: 80\n",
      "0.0013994945\n",
      "0.0014597302\n",
      "0.0015136638\n",
      "0.0012797404\n",
      "Epoch: 81\n",
      "0.0014498163\n",
      "0.0013333339\n",
      "0.0014304694\n",
      "0.0013914515\n",
      "Epoch: 82\n",
      "0.0013044509\n",
      "0.0014076023\n",
      "0.00140154\n",
      "0.001360531\n",
      "Epoch: 83\n",
      "0.001205643\n",
      "0.0012447758\n",
      "0.0015661964\n",
      "0.0012649634\n",
      "Epoch: 84\n",
      "0.0014508312\n",
      "0.0011914894\n",
      "0.0012278692\n",
      "0.0013165859\n",
      "Epoch: 85\n",
      "0.0012486491\n",
      "0.0012988743\n",
      "0.0011075006\n",
      "0.0016518434\n",
      "Epoch: 86\n",
      "0.0013555664\n",
      "0.0011479827\n",
      "0.0012359077\n",
      "0.0011294759\n",
      "Epoch: 87\n",
      "0.0010351454\n",
      "0.0012422802\n",
      "0.0013791865\n",
      "0.001087058\n",
      "Epoch: 88\n",
      "0.0011842374\n",
      "0.0012356053\n",
      "0.001207254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:02<00:00, 37.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000894703\n",
      "Epoch: 89\n",
      "0.0010939206\n",
      "0.0011869469\n",
      "0.0010726748\n",
      "0.0014667008\n",
      "Epoch: 90\n",
      "0.0011192248\n",
      "0.0011360352\n",
      "0.0010307959\n",
      "0.0014110586\n",
      "Epoch: 91\n",
      "0.0012450226\n",
      "0.0011255438\n",
      "0.0010372499\n",
      "0.00081452134\n",
      "Epoch: 92\n",
      "0.0010338356\n",
      "0.001123039\n",
      "0.0010743053\n",
      "0.0011081869\n",
      "Epoch: 93\n",
      "0.0011755385\n",
      "0.0009302222\n",
      "0.0010919616\n",
      "0.0009621973\n",
      "Epoch: 94\n",
      "0.001166193\n",
      "0.0009117643\n",
      "0.0009737624\n",
      "0.0011640359\n",
      "Epoch: 95\n",
      "0.0010410668\n",
      "0.0010441467\n",
      "0.00095650007\n",
      "0.0010310569\n",
      "Epoch: 96\n",
      "0.0010060468\n",
      "0.00094787957\n",
      "0.0010479729\n",
      "0.00090601266\n",
      "Epoch: 97\n",
      "0.0010003975\n",
      "0.0009573049\n",
      "0.0009649473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 100/100 [00:02<00:00, 37.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009888591\n",
      "Epoch: 98\n",
      "0.000882193\n",
      "0.0009075213\n",
      "0.0010204638\n",
      "0.0011050627\n",
      "Epoch: 99\n",
      "0.00081811537\n",
      "0.0009629692\n",
      "0.0009432935\n",
      "0.001120022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "loss_hist=[]\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    \n",
    "    # iterator.initalizer를 반복문 안에 넣어줘야 배치를 다 꺼내 쓴 다음에 다시 배치를 쓸 수 있다. 이것을 반복문 바깥에 두면 계속 OutOfRangeError 에러가 난다.\n",
    "    sess.run(iterator.initializer, feed_dict = {encoder_input: encoder_data, decoder_input: decoder_data, \n",
    "                                                targets: targets_data, encoder_seq_len: encoder_sequence_length, decoder_seq_len: decoder_sequence_length})\n",
    "    print(\"Epoch: %s\" %epoch)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            _, loss = sess.run([train_op, object2text.seq2seq_loss])\n",
    "            print(loss)            \n",
    "            \n",
    "        # 에러가 발생한다는 것은 더 이상 꺼내 올 배치가 없다는 것이다. 따라서 배치를 꺼내오는 while문을 종료하고 다음 epoch로 넘어간다.\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 학습된 가중치가 저장된 sess를 통해서 inference를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = object2text.infer(sess, btc_enc_inp=encoder_data, btc_enc_seq_len=encoder_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  5  4  0 10 14  0]\n",
      " [ 7  9 12 10 14  0  0]\n",
      " [ 7  5  4  0  3 14  0]\n",
      " [ 7  9 12  3 14  0  0]\n",
      " [ 7  5  4  0  2  6 14]\n",
      " [ 7  9 12  2  6 14  0]\n",
      " [ 7  5  4  0  8 11 14]\n",
      " [ 7  9 12  8 11 14  0]\n",
      " [ 7  5  4  0  1 14  0]\n",
      " [ 7  9 12  1 14  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plane', 'take', 'off', 'from', 'dallas', '<end>', 'from'],\n",
       " ['plane', 'land', 'at', 'dallas', '<end>', 'from', 'from'],\n",
       " ['plane', 'take', 'off', 'from', 'chicago', '<end>', 'from'],\n",
       " ['plane', 'land', 'at', 'chicago', '<end>', 'from', 'from'],\n",
       " ['plane', 'take', 'off', 'from', 'los', 'angeles', '<end>'],\n",
       " ['plane', 'land', 'at', 'los', 'angeles', '<end>', 'from'],\n",
       " ['plane', 'take', 'off', 'from', 'new', 'york', '<end>'],\n",
       " ['plane', 'land', 'at', 'new', 'york', '<end>', 'from'],\n",
       " ['plane', 'take', 'off', 'from', 'miami', '<end>', 'from'],\n",
       " ['plane', 'land', 'at', 'miami', '<end>', 'from', 'from']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda row: [idx2word[idx] for idx in row], prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
